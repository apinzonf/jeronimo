<?xml version="1.0"?>
<b:Sources SelectedStyle="" xmlns:b="http://schemas.openxmlformats.org/officeDocument/2006/bibliography" xmlns="http://schemas.openxmlformats.org/officeDocument/2006/bibliography"><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>Theobalt2004</b:Tag><b:Title>Marker-free Kinematic Skeleton Estimation from Sequences of Volume Data</b:Title><b:Year>2004</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Theobalt</b:Last><b:First>Christian</b:First></b:Person><b:Person><b:Last>Aguiar</b:Last><b:Middle>de</b:Middle><b:First>Edilson</b:First></b:Person><b:Person><b:Last>Magnor</b:Last><b:First>Marcus</b:First></b:Person><b:Person><b:Last>Theisel</b:Last><b:First>Holger</b:First></b:Person><b:Person><b:Last>Seidel</b:Last><b:First>Hans-Peter</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>57-64</b:Pages><b:Publisher>ACM</b:Publisher><b:BookTitle>ACM Symposium on Virtual Reality Software and Technology (VRST 2004)</b:BookTitle><b:ConferenceName>Association of Computing Machinery (ACM)</b:ConferenceName><b:BIBTEX_Abstract>For realistic animation of an artificial character a body model that represents the character's kinematic structure is required. Hierarchical skeleton models are widely used which represent bodies as chains of bones with interconnecting joints. In video motion capture, animation parameters are derived from the performance of a subject in the real world. For this acquisition procedure too, a kinematic body model is required. Typically, the generation of such a model for tracking and animation is, at best, a semi-automatic process. We present a novel approach that estimates a hierarchical skeleton model of an arbitrary moving subject from sequences of voxel data that were reconstructed from multi-view video footage. Our method does not require a-priori information about the body structure. We demonstrate its performance using synthetic and real data.</b:BIBTEX_Abstract></b:Source><b:Source><b:SourceType>JournalArticle</b:SourceType><b:Tag>Chen2008</b:Tag><b:Title>Connection skeleton extraction based on contour connectedness</b:Title><b:Year>2008</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Chen</b:Last><b:First>Mang</b:First></b:Person><b:Person><b:Last>Liu</b:Last><b:First>Yun-cai</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>521-527</b:Pages><b:Volume>13</b:Volume><b:JournalName>Journal of Shanghai Jiaotong University (Science)</b:JournalName><b:BIBTEX_Abstract>Abstract&amp;nbsp;&amp;nbsp;A stable skeleton is very important to some applications such as vehicle navigation, object represent and pattern recognition. The connection skeleton is just one that not only can be computed stably but also can figure the connectivity structure of contour. A new method named continuous connectivity detection and a new model named approximate regular polygon (ARP) were proposed for connection skeleton extraction. Both the method and the model were tested by the real maps of road network including flyovers, interchanges and other common object contours. Satisfactory results were obtained.</b:BIBTEX_Abstract></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>Shu2008</b:Tag><b:Title>Hardware-based camera calibration and 3D modelling under circular motion</b:Title><b:Year>2008</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Shu</b:Last><b:First>Bo</b:First></b:Person><b:Person><b:Last>Qiu</b:Last><b:First>Xianjie</b:First></b:Person><b:Person><b:Last>Wang</b:Last><b:First>Zhaoqi</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>1-6</b:Pages><b:BookTitle>Computer Vision and Pattern Recognition Workshops</b:BookTitle><b:JournalName>Computer Vision and Pattern Recognition Workshops, 2008. CVPRW '08. IEEE Computer Society Conference on</b:JournalName><b:BIBTEX_Abstract>In this paper, we present a combined camera calibration and image based modeling method using an iterative optimization of shape from silhouette under circular motion. By minimizing the difference between the projections of reconstructed visual hull and the silhouette images using graphics hardware, the optimization can finally converge to accurate camera parameters and realistic visual hull efficiently and robustly. Using this method, we can automatically create photorealistic 3D models directly from images.</b:BIBTEX_Abstract><b:BIBTEX_KeyWords>image motion analysis, image reconstruction, image sensors, optimisation, realistic images, solid modellingcircular motion, graphics hardware, hardware-based camera calibration, image based modeling method, iterative optimization, photorealistic 3D modelling, reconstructed visual hull, silhouette image</b:BIBTEX_KeyWords></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>Slyper2008</b:Tag><b:Title>Action Capture with Accelerometers</b:Title><b:Year>2008</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Slyper</b:Last><b:First>Ronit</b:First></b:Person><b:Person><b:Last>Hodgins</b:Last><b:First>Jessica</b:First></b:Person></b:NameList></b:Author></b:Author><b:BookTitle>2008 ACM SIGGRAPH / Eurographics Symposium on Computer Animation</b:BookTitle><b:BIBTEX_Abstract>We create a performance animation system that leverages the power of low-cost accelerometers, readily available motion capture databases, and construction techniques from e-textiles. Our system, built with only off-the-shelf parts, consists of five accelerometers sewn into a comfortable shirt that streams data to a computer. The accelerometer readings are continuously matched against accelerations computed from existing motion capture data, and an avatar is animated with the closest match. We evaluate our system visually and using simultaneous motion and accelerometer capture.</b:BIBTEX_Abstract></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>HernandezEsteban2002</b:Tag><b:Title>Multi-stereo 3D object reconstruction</b:Title><b:Year>2002</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Esteban</b:Last><b:Middle>Hernandez</b:Middle><b:First>C.</b:First></b:Person><b:Person><b:Last>Schmitt</b:Last><b:First>F.</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>159-166</b:Pages><b:BookTitle>3D Data Processing Visualization and Transmission</b:BookTitle><b:JournalName>3D Data Processing Visualization and Transmission, 2002. Proceedings. First International Symposium on</b:JournalName><b:BIBTEX_Abstract>We present a method for the reconstruction of a 3D real object from a sequence of high-definition images. We combine two different procedures: a shape from silhouette technique which provides a coarse 3D initial model followed by a multi-stereo carving technique. We propose a fast but accurate method for the estimation of the carving depth at each vertex of the 3D mesh. The quality of the final textured 3D reconstruction models allows us to validate the method.</b:BIBTEX_Abstract><b:BIBTEX_KeyWords> computer graphics, image reconstruction, image resolution, image sequences, image texture, mesh generation, robot vision, stereo image processing 3D mesh vertex, 3D object reconstruction, 3D real object, carving depth estimation, coarse initial model, computer graphics, high-definition image sequence, multi-stereo carving, robot vision, shape from silhouette technique, textured 3D reconstruction models</b:BIBTEX_KeyWords></b:Source><b:Source><b:SourceType>Misc</b:SourceType><b:Tag>Hasinoff2006</b:Tag><b:Title>Confocal Stereo</b:Title><b:Year>2006</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Hasinoff</b:Last><b:First>Samuel</b:First></b:Person><b:Person><b:Last>Kutulakos</b:Last><b:First>Kiriakos</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>620-634</b:Pages><b:JournalName>Computer Vision â€“ ECCV 2006</b:JournalName><b:PublicationTitle>Confocal Stereo</b:PublicationTitle><b:BIBTEX_Abstract>We present confocal stereo, a new method for computing 3D shape by controlling the focus and aperture of a lens. The method is specifically designed for reconstructing scenes with high geometric complexity or fine-scale texture. To achieve this, we introduce the confocal constancy property, which states that as the lens aperture varies, the pixel intensity of a visible in-focus scene point will vary in a scene-independent way, that can be predicted by prior radiometric lens calibration. The only requirement is that incoming radiance within the cone subtended by the largest aperture is nearly constant. First, we develop a detailed lens model that factors out the distortions in high resolution SLR cameras (12MP or more) with large-aperture lenses (e.g., f1.2). This allows us to assemble an A F aperture-focus image (AFI) for each pixel, that collects the undistorted measurements over all A apertures and F focus settings. In the AFI representation, confocal constancy reduces to color comparisons within regions of the AFI, and leads to focus metrics that can be evaluated separately for each pixel. We propose two such metrics and present initial reconstruction results for complex scenes.</b:BIBTEX_Abstract></b:Source><b:Source><b:SourceType>JournalArticle</b:SourceType><b:Tag>Yue2008</b:Tag><b:Title>Synthesis of Silhouettes and Visual Hull Reconstruction for Articulated Humans</b:Title><b:Year>2008</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Yue</b:Last><b:First>Zhanfeng</b:First></b:Person><b:Person><b:Last>Chellappa</b:Last><b:First>R.</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>1565-1577</b:Pages><b:Volume>10</b:Volume><b:JournalName>Multimedia, IEEE Transactions on</b:JournalName><b:BIBTEX_Abstract>In this paper, we propose a complete framework for improved synthesis and understanding of the human pose from a limited number of silhouette images. It combines the active image-based visual hull (IBVH) algorithm and a contour-based body part segmentation technique. We derive a simple, approximate algorithm to decide the extrinsic parameters of a virtual camera, and synthesize the turntable image collection of the person using the IBVH algorithm by actively moving the virtual camera on a properly computed circular trajectory around the person. Using the turning function distance as the silhouette similarity measurement, this approach can be used to generate the desired pose-normalized images for recognition applications. In order to overcome the inability of the visual hull (VH) method to reconstruct concave regions, we propose a contour-based human body part localization algorithm to segment the silhouette images into convex body parts. The body parts observed from the virtual view are generated separately from the corresponding body parts observed from the input views and then assembled together for a more accurate VH reconstruction. Furthermore, the obtained turntable image collection helps to improve the body part segmentation and identification process. By using the inner distance shape context (IDSC) measurement, we are able to estimate the body part locations more accurately from a synthesized view where we can localize the body part more precisely. Experiments show that the proposed algorithm can greatly improve body part segmentation and hence shape reconstruction results.</b:BIBTEX_Abstract><b:BIBTEX_KeyWords>approximation theory, cameras, edge detection, image reconstruction, image segmentation, pose estimation, shape recognition, virtual realityactive image, approximate algorithm, articulated human pose, circular trajectory computation, contour-based body part segmentation technique, human body part localization algorithm, inner distance shape context measurement, silhouette image synthesis, silhouette similarity measurement, turning function distance, turntable image collection, virtual camera, visual hull reconstruction</b:BIBTEX_KeyWords></b:Source><b:Source><b:SourceType>Report</b:SourceType><b:BIBTEX_Entry>mastersthesis</b:BIBTEX_Entry><b:Tag>Aguiar2003</b:Tag><b:Title>Character Animation from a Motion Capture Database</b:Title><b:Year>2003</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Aguiar</b:Last><b:Middle>de</b:Middle><b:First>Edilson</b:First></b:Person></b:NameList></b:Author></b:Author><b:Department>Universit{\"a}t des Saarlandes</b:Department><b:BIBTEX_Abstract>With the advent of photo-realism in Computer Graphics, life-like character animations that capture fine details of a motion have become more important. We have studied methods [1] that use the information contained in a motion capture database to assist in the creation of a realistic character animation. Starting with an animation sketch, where only a small number of keyframes for some degrees of freedom are set, the motion capture data is used to enhance the initial motion.</b:BIBTEX_Abstract></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>Landabaso2008</b:Tag><b:Title>Shape from inconsistent silhouette for free viewpoint video</b:Title><b:Year>2008</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Landabaso</b:Last><b:First>J.-L.</b:First></b:Person><b:Person><b:Last>Lizcano</b:Last><b:First>L.</b:First></b:Person><b:Person><b:Last>Pardas</b:Last><b:First>M.</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>213-216</b:Pages><b:BookTitle>Image Processing</b:BookTitle><b:JournalName>Image Processing, 2008. ICIP 2008. 15th IEEE International Conference on</b:JournalName><b:BIBTEX_Abstract>In this paper we present an efficient image-based rendering algorithm that obtains novel images from a set of views of the scene of interest. The approach described uses silhouette image data to compute the Visual Hull, the largest volume that is compatible with the silhouettes that delimit the objects of interest. Since the Visual Hull is not explicitly computed, this approach does not suffer from the quantization artifacts of volumetric approaches. In contrast to previous works, we explore how detection errors in the silhouettes affect the novel rendered view and propose a method to detect errors in the original silhouettes based on the consistency principle that states that the projection of the Visual Hull should exactly correspond with original silhouettes.</b:BIBTEX_Abstract><b:BIBTEX_KeyWords>rendering (computer graphics), video signal processingerror detection, free viewpoint video, image-based rendering algorithm, immersive videoconferencing system, inconsistent silhouette image, quantization artifacts, silhouette image data, visual hull</b:BIBTEX_KeyWords></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>Shalom2008</b:Tag><b:Title>Part Analogies in Sets of Objects</b:Title><b:Year>2008</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Shalom</b:Last><b:First>Shy</b:First></b:Person><b:Person><b:Last>Shapira</b:Last><b:First>Lior</b:First></b:Person><b:Person><b:Last>Shamir</b:Last><b:First>Ariel</b:First></b:Person><b:Person><b:Last>Cohen-Or</b:Last><b:First>Daniel</b:First></b:Person></b:NameList></b:Author></b:Author><b:BookTitle>Eurographics Workshop on 3D Object Retrieval ‘08</b:BookTitle><b:BIBTEX_Abstract>Shape retrieval can benefit from analogies among similar shapes and parts of different objects. By partitioning an object to meaningful parts and finding analogous parts in other objects, sub-parts and partial match queries can be utilized. First by searching for similar parts in the context of their shape, and second by finding similarities even among objects that differ in their general shape and topology. Moreover, analogies can create the basis for semantic text-based searches: for instance, in this paper we demonstrate a simple annotation tool that carries tags of object parts from one model to many others using analogies. We partition 3D objects based on the shape-diameter function (SDF), and use it to find corresponding parts in other objects. We present results on finding analogies among numerous objects from shape repositories, and demonstrate sub-part queries using an implementation of a simple search and retrieval application.</b:BIBTEX_Abstract></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>Tierny2008</b:Tag><b:Title>Fast and precise kinematic skeleton extraction of 3D dynamic meshes</b:Title><b:Year>2008</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Tierny</b:Last><b:First>J.</b:First></b:Person><b:Person><b:Last>Vandeborre</b:Last><b:First>J.-P.</b:First></b:Person><b:Person><b:Last>Daoudi</b:Last><b:First>M.</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>1-4</b:Pages><b:BookTitle>Pattern Recognition</b:BookTitle><b:JournalName>Pattern Recognition, 2008. ICPR 2008. 19th International Conference on</b:JournalName><b:BIBTEX_Abstract>Shape skeleton extraction is a fundamental pre-processing task in shape-based pattern recognition. This paper presents a new algorithm for fast and precise extraction of kinematic skeletons of 3D dynamic surface meshes. Unlike previous approaches, surface motions are characterized by the mesh edge-length deviation induced by its transformation through time. Then a static skeleton extraction algorithm based on Reeb graphs exploits this latter information to extract the kinematic skeleton. This hybrid static and dynamic shape analysis enables the precise detection of objectspsila articulations as well as shape topological transitions corresponding to possibly-articulated immobile objectspsila features. Experiments show that the proposed algorithm is faster than previous techniques and still achieves better accuracy.</b:BIBTEX_Abstract><b:BIBTEX_KeyWords>computational geometry, graph theory, mesh generation, pattern recognition3D dynamic meshes, 3D dynamic surface meshes, Reeb graphs, dynamic shape analysis, fast kinematic skeleton extraction, mesh edge-length deviation, possibly-articulated immobile objects features, precise kinematic skeleton extraction, preprocessing task, shape skeleton extraction, shape topological transitions, shape-based pattern recognition, static shape analysis, static skeleton extraction algorithm, surface motions</b:BIBTEX_KeyWords></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>Cheung2003</b:Tag><b:Title>Shape-from-silhouette of articulated objects and its use for human body kinematics estimation and motion capture</b:Title><b:Year>2003</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Cheung</b:Last><b:First>K.M.G.</b:First></b:Person><b:Person><b:Last>Baker</b:Last><b:First>S.</b:First></b:Person><b:Person><b:Last>Kanade</b:Last><b:First>T.</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages> I-77-I-84 vol.1</b:Pages><b:Volume>1</b:Volume><b:BookTitle>Computer Vision and Pattern Recognition, 2003. Proceedings. 2003 IEEE Computer Society Conference on</b:BookTitle><b:JournalName>Computer Vision and Pattern Recognition, 2003. Proceedings. 2003 IEEE Computer Society Conference on</b:JournalName><b:BIBTEX_Abstract> Shape-from-silhouette (SFS), also known as visual hull (VH) construction, is a popular 3D reconstruction method, which estimates the shape of an object from multiple silhouette images. The original SFS formulation assumes that the entire silhouette images are captured either at the same time or while the object is static. This assumption is violated when the object moves or changes shape. Hence the use of SFS with moving objects has been restricted to treating each time instant sequentially and independently. Recently we have successfully extended the traditional SFS formulation to refine the shape of a rigidly moving object over time. We further extend SFS to apply to dynamic articulated objects. Given silhouettes of a moving articulated object, the process of recovering the shape and motion requires two steps: (1) correctly segmenting (points on the boundary of) the silhouettes to each articulated part of the object, (2) estimating the motion of each individual part using the segmented silhouette. In this paper, we propose an iterative algorithm to solve this simultaneous assignment and alignment problem. Once we have estimated the shape and motion of each part of the object, the articulation points between each pair of rigid parts are obtained by solving a simple motion constraint between the connected parts. To validate our algorithm, we first apply it to segment the different body parts and estimate the joint positions of a person. The acquired kinematic (shape and joint) information is then used to track the motion of the person in new video sequences.</b:BIBTEX_Abstract><b:BIBTEX_KeyWords> edge detection, image reconstruction, image segmentation, image sequences, iterative methods, kinematics, motion estimation, object detection, optical tracking, stereo image processing, video signal processing 3D reconstruction, dynamic articulated object, human body kinematics estimation, iterative algorithm, joint position estimation, motion capture, motion estimation, motion recovery, motion tracking, moving articulated object, multiple silhouette images, object shape estimation, shape recovery, shape-from-silhouette, silhouette image segmentation, video sequence, visual hull construction</b:BIBTEX_KeyWords></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>Hoppe1996</b:Tag><b:Title>Progressive meshes</b:Title><b:Year>1996</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Hoppe</b:Last><b:First>Hugues</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>99-108</b:Pages><b:Publisher>ACM</b:Publisher><b:BookTitle>SIGGRAPH '96: Proceedings of the 23rd annual conference on Computer graphics and interactive techniques</b:BookTitle><b:BIBTEX_Abstract>Highly detailed geometric models are rapidly becoming commonplace in computer graphics. These models, often represented as complex triangle meshes, challenge rendering performance, transmission bandwidth, and storage capacities. This paper introduces the progressive mesh (PM) representation, a new scheme for storing and transmitting arbitrary triangle meshes. This efficient, lossless, continuous-resolution representation addresses several practical problems in graphics: smooth geomorphing of level-of-detail approximations, progressive transmission, mesh compression, and selective refinement.
In addition, we present a new mesh simplification procedure for constructing a PM representation from an arbitrary mesh. The goal of this optimization procedure is to preserve not just the geometry of the original mesh, but more importantly its overall appearance as defined by its discrete and scalar appearance attributes such as material identifiers, color values, normals, and texture coordinates. We demonstrate construction of the PM representation and its applications using several practical models.</b:BIBTEX_Abstract></b:Source><b:Source><b:SourceType>Report</b:SourceType><b:BIBTEX_Entry>phdthesis</b:BIBTEX_Entry><b:Tag>KryPhd05</b:Tag><b:Title>Interaction Capture and Synthesis of Human Hands</b:Title><b:Year>2005</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Kry</b:Last><b:Middle>G.</b:Middle><b:First>Paul</b:First></b:Person></b:NameList></b:Author></b:Author><b:Department>University of British Columbia</b:Department><b:BIBTEX_Abstract>This thesis addresses several issues in modelling interaction with human hands in computer graphics and animation. Modifying motion capture to satisfy the constraints of new animation is difficult when contact is involved because physical interaction involves energy or power transfer between the system of interest and the environment, and is a critical problem for computer animation of hands. Although contact force measurements provide a means of monitoring this transfer, motion capture as currently used for creating animation has largely ignored contact forces. We present a system of capturing synchronized motion and contact forces, called interaction capture. We transform interactions such as grasping into joint compliances and a nominal reference trajectory in an approach inspired by the equilibrium point hypothesis of human motor control. New interactions are synthesized through simulation of a quasi-static compliant articulated model in a dynamic environment that includes friction. This uses a novel position-based linear complementarity problem formulation that includes friction, breaking contact, and coupled compliance between contacts at different fingers. We present methods for reliable interaction capture, addressing calibration, force estimation, and synchronization. Additionally, although joint compliances are traditionally estimated with perturbation-based methods, we introduce a technique that instead produces estimates without perturbation. We validate our results with data from previous work and our own perturbation-based estimates. A complementary goal of this work is hand-based interaction in virtual environments. We present techniques for whole-hand interaction using the Tango, a novel sensor that performs interaction capture by measuring pressure images and accelerations. We approximate grasp hand-shapes from previously observed data through rotationally invariant comparison of pressure measurements. We also introduce methods involving heuristics and thresholds that make reliable drift-free navigation possible with the Tango. Lastly, rendering the skin deformations of articulated characters is a fundamental problem for computer animation of hands. We present a deformation model, called EigenSkin, which provides a means of rendering physically- or example-based deformation models at interactive rates on graphics hardware.</b:BIBTEX_Abstract></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>Zhang2008</b:Tag><b:Title>Capturing Images with Sparse Informational Pixels using Projected 3D Tags</b:Title><b:Year>2008</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Zhang</b:Last><b:First>L.</b:First></b:Person><b:Person><b:Last>Subramaniam</b:Last><b:First>N.</b:First></b:Person><b:Person><b:Last>Lin</b:Last><b:First>R.</b:First></b:Person><b:Person><b:Last>Nayar</b:Last><b:Middle>K.</b:Middle><b:First>S.</b:First></b:Person><b:Person><b:Last>Raskar</b:Last><b:First>R.</b:First></b:Person></b:NameList></b:Author></b:Author><b:BookTitle>Proceedings of IEEE Virtual Reality</b:BookTitle><b:BIBTEX_Abstract>In this paper, we propose a novel imaging system that enables the capture of photos and videos with sparse informational pixels. Our system is based on the projection and detection of 3D optical tags. We use an infrared (IR) projector to project temporally-coded (blinking) dots onto selected points in a scene. These tags are invisible to the human eye, but appear as clearly visible time-varying codes to an IR photosensor. As a proof of concept, we have built a prototype camera system (consisting of co-located visible and IR sensors) to simultaneously capture visible and IR images. When a user takes an image of a tagged scene using such a camera system, all the scene tags that are visible from the system's viewpoint are detected. In addition, tags that lie in the field of view but are occluded, and ones that lie just outside the field of view, are also automatically generated for the image. Associated with each tagged pixel is its 3D location and the identity of the object that the tag falls on. Our system can interface with conventional image recognition methods for efficient scene authoring, enabling objects in an image to be robustly identified using cheap cameras, minimal computations, and no domain knowledge. We demonstrate several applications of our system, including, photo-browsing, e-commerce, augmented reality, and objection localization.</b:BIBTEX_Abstract></b:Source><b:Source><b:SourceType>JournalArticle</b:SourceType><b:Tag>Zou2009</b:Tag><b:Title>Automatic reconstruction of 3D human motion pose from uncalibrated monocular video sequences based on markerless human motion tracking</b:Title><b:Year>2009</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Zou</b:Last><b:First>Beiji</b:First></b:Person><b:Person><b:Last>Chen</b:Last><b:First>Shu</b:First></b:Person><b:Person><b:Last>Shi</b:Last><b:First>Cao</b:First></b:Person><b:Person><b:Last>Providence</b:Last><b:Middle>Marie</b:Middle><b:First>Umugwaneza</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages> - </b:Pages><b:Volume>In Press, Corrected Proof</b:Volume><b:JournalName>Pattern Recognition</b:JournalName><b:BIBTEX_Abstract>We present a method to reconstruct human motion pose from uncalibrated monocular video sequences based on the morphing appearance model matching. The human pose estimation is made by integrated human joint tracking with pose reconstruction in depth-first order. Firstly, the Euler angles of joint are estimated by inverse kinematics based on human skeleton constrain. Then, the coordinates of pixels in the body segments in the scene are determined by forward kinematics, by projecting these pixels in the scene onto the image plane under the assumption of perspective projection to obtain the region of morphing appearance model in the image. Finally, the human motion pose can be reconstructed by histogram matching. The experimental results show that this method can obtain favorable reconstruction results on a number of complex human motion sequences.</b:BIBTEX_Abstract><b:BIBTEX_KeyWords>3D human motion reconstruction</b:BIBTEX_KeyWords></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>Ma2003</b:Tag><b:Title>Skeleton Extraction of 3D Objects with Radial Basis Functions</b:Title><b:Year>2003</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Ma</b:Last><b:First>Wan-Chun</b:First></b:Person><b:Person><b:Last>Wu</b:Last><b:First>Fu-Che</b:First></b:Person><b:Person><b:Last>Ouhyoung</b:Last><b:First>Ming</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>207</b:Pages><b:Publisher>IEEE Computer Society</b:Publisher><b:BookTitle>SMI '03: Proceedings of the Shape Modeling International 2003</b:BookTitle><b:BIBTEX_Abstract>Skeleton is a lower dimensional shape description of anobject. The requirements of a skeleton differ with applications.For example, object recognition requires skele-tonswith primitive shape features to make similarity comparison.On the other hand, surface reconstruction needsskeletons which contain detailed geometry information toreduce the approximation error in the reconstruction process.Whereas many previous works are concerned aboutskeleton extraction, most of these methods are sensitive tonoise, time consuming, or restricted to specific 3D models.A practical approach for extracting skeletons from general3D models using radial basis functions (RBFs) is proposed.Skeleton generated with this approach conformsmore to the human perception. Given a 3D polygonalmodel, the vertices are regarded as centers for RBF level setconstruction. Next, a gradient descent algorithm is appliedto each vertex to locate the local maxima in the RBF; thegradient is calculated directly from the partial derivativesof the RBF. Finally, with the inherited connectivity from theoriginal model, local maximum pairs are connected withlinks driven by the active contour model. The skeletonizationprocess is completed when the potential energy of theselinks is minimized.</b:BIBTEX_Abstract></b:Source><b:Source><b:SourceType>JournalArticle</b:SourceType><b:Tag>Liu2008</b:Tag><b:Title>Surface Reconstruction From Non-parallel Curve Networks</b:Title><b:Year>2008</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Liu</b:Last></b:Person><b:Person><b:Last>L.</b:Last></b:Person><b:Person><b:Last>Bajaj</b:Last></b:Person><b:Person><b:Last>C.</b:Last></b:Person><b:Person><b:Last>Deasy</b:Last></b:Person><b:Person><b:Last>O.</b:Last><b:First>J.</b:First></b:Person><b:Person><b:Last>Low</b:Last></b:Person><b:Person><b:Last>A.</b:Last><b:First>D.</b:First></b:Person><b:Person><b:Last>Ju</b:Last></b:Person><b:Person><b:Last>T.</b:Last></b:Person></b:NameList></b:Author></b:Author><b:Pages>155-163</b:Pages><b:Volume>27</b:Volume><b:Publisher>Blackwell Publishing</b:Publisher><b:JournalName>Computer Graphics Forum</b:JournalName><b:BIBTEX_Abstract>Building surfaces from cross-section curves has wide applications including bio-medical modeling. Previous work in this area has mostly focused on connecting simple closed curves on parallel cross-sections. Here we consider the more general problem where input data may lie on non-parallel cross-sections and consist of curve networks that represent the segmentation of the underlying object by different material or tissue types (e.g., skin, muscle, bone, etc.) on each cross-section. The desired output is a surface network that models both the exterior surface and the internal partitioning of the object. We introduce an algorithm that is capable of handling curve networks of arbitrary shape and topology on cross-section planes with arbitrary orientations. Our algorithm is simple to implement and is guaranteed to produce a closed surface network that interpolates the curve network on each cross-section. Our method is demonstrated on both synthetic and bio-medical examples.</b:BIBTEX_Abstract></b:Source><b:Source><b:SourceType>JournalArticle</b:SourceType><b:Tag>Mercier2005</b:Tag><b:Title>Shape from Silhouette: Image Pixels for Marching Cubes</b:Title><b:Year>2005</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Mercier</b:Last><b:First>B.</b:First></b:Person><b:Person><b:Last>Meneveaux</b:Last><b:First>D.</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>112-118</b:Pages><b:Volume>13</b:Volume><b:JournalName>Journal of WSCG'2005</b:JournalName><b:BIBTEX_Abstract>In this paper, we propose to use image pixels for geometry reconstruction with a shape from silhouette approach.
We aim at estimating shape and normal for the surface of a single object seen through calibrated images. From the
voxel-based shape obtained with the algorithm proposed by R. Szeliski in [18], our main contribution concerns the
use of image pixels together with marching cubes for constructing a triangular mesh. We also provide a mean for
estimating a normal inside each voxel with two different methods: (i) using marching cubes triangles and (ii) using
only voxels. As seen in the results, our method proves accurate even for real objects acquired with a usual camera
and an inexpensive acquisition system</b:BIBTEX_Abstract></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>starck06volumetric</b:Tag><b:Title>Volumetric stereo with silhouette and feature constraints</b:Title><b:Year>2006</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Starck</b:Last><b:First>J.</b:First></b:Person><b:Person><b:Last>Hilton</b:Last><b:First>A.</b:First></b:Person><b:Person><b:Last>Miller</b:Last><b:First>G.</b:First></b:Person></b:NameList></b:Author></b:Author><b:BookTitle>British Machine Vision Conference</b:BookTitle><b:BIBTEX_Abstract>This paper presents a novel volumetric reconstruction technique that combines shape-from-silhouette with stereo photo-consistency in a global optimisation that enforces feature constraints across multiple views. Human
shape reconstruction is considered where extended regions of uniform appearance, complex self-occlusions and sparse feature cues represent a challenging problem for conventional reconstruction techniques. A uni?ed approach is introduced to ?rst reconstruct the occluding contours and left-right consistent edge contours in a scene and then incorporate these contour constraints in a global surface optimisation using graph-cuts. The proposed technique maximises photo-consistency on the surface, while satisfying silhouette constraints to provide shape in the presence of uniform surface appearance and edge feature constraints to align key image features across views.</b:BIBTEX_Abstract><b:BIBTEX_KeyWords>3d, graphcut</b:BIBTEX_KeyWords></b:Source><b:Source><b:SourceType>JournalArticle</b:SourceType><b:Tag>Bottino2004</b:Tag><b:Title>The visual hull of smooth curved objects</b:Title><b:Year>2004</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Bottino</b:Last><b:First>A.</b:First></b:Person><b:Person><b:Last>Laurentini</b:Last><b:First>A.</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>1622-1632</b:Pages><b:Volume>26</b:Volume><b:JournalName>Pattern Analysis and Machine Intelligence, IEEE Transactions on</b:JournalName><b:BIBTEX_Abstract> The visual hull is a geometric entity that relates the shape of an object to its silhouettes or shadows. This paper develops the theory of the visual hull of generic smooth objects. We show that the visual hull can be constructed using surfaces which partition the viewpoint space of the aspect graph of the object. The surfaces are those generated by the visual events tangent crossing and triple point. An analysis based on the shape of the object at the tangency points of these surfaces allows pruning away many surfaces and patches not relevant to the construction. An algorithm for computing the visual hull is outlined.</b:BIBTEX_Abstract><b:BIBTEX_KeyWords> computer vision, geometry, graph theory, image reconstruction computer vision, generic smooth objects, geometric entity, graph theory, image reconstruction, object silhouettes, smooth curved objects, visual events, visual hull computing</b:BIBTEX_KeyWords></b:Source><b:Source><b:SourceType>JournalArticle</b:SourceType><b:Tag>Xue2009</b:Tag><b:Title>Feature fusion for basic behavior unit segmentation from video sequences</b:Title><b:Year>2009</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Xue</b:Last><b:First>Xinwei</b:First></b:Person><b:Person><b:Last>Henderson</b:Last><b:Middle>C.</b:Middle><b:First>Thomas</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>239-248</b:Pages><b:Volume>57</b:Volume><b:Publisher>North-Holland Publishing Co.</b:Publisher><b:JournalName>Robot. Auton. Syst.</b:JournalName><b:BIBTEX_Abstract>It has become increasingly popular to study animal behaviors with the assistance of video recordings. An automated video processing and behavior analysis system is desired to replace the traditional manual annotation. We propose a framework for automatic video based behavior analysis systems, which consists of four major modules: behavior modeling, feature extraction from video sequences, basic behavior unit (BBU) discovery and complex behavior recognition. BBU discovery is performed based on features extracted from video sequences, hence the fusion of multiple dimensional features is very important. In this paper, we explore the application of feature fusion techniques to BBU discovery with one and multiple cameras. We applied the vector fusion (SBP) method, a multi-variate vector visualization technique, in fusing the features obtained from a single camera. This technique reduces the multiple dimensional data into two dimensional (SBP) space, and the spatial and temporal analysis in SBP space can help discover the underlying data groups. Then we present a simple feature fusion technique for BBU discovery from multiple cameras with the affinity graph method. Finally, we present encouraging results on a physical system and a synthetic mouse-in-a-cage scenario from one, two, and three cameras. The feature fusion methods in this paper are simple yet effective.</b:BIBTEX_Abstract></b:Source><b:Source><b:SourceType>JournalArticle</b:SourceType><b:Tag>Yamazaki2009</b:Tag><b:Title>The Theory and Practice of Coplanar Shadowgram Imaging for Acquiring Visual Hulls of Intricate Objects</b:Title><b:Year>2009</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Yamazaki</b:Last><b:First>Shuntaro</b:First></b:Person><b:Person><b:Last>Narasimhan</b:Last><b:Middle>G.</b:Middle><b:First>Srinivasa</b:First></b:Person><b:Person><b:Last>Baker</b:Last><b:First>Simon</b:First></b:Person><b:Person><b:Last>Kanade</b:Last><b:First>Takeo</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>259-280</b:Pages><b:Volume>81</b:Volume><b:Publisher>Kluwer Academic Publishers</b:Publisher><b:JournalName>Int. J. Comput. Vision</b:JournalName><b:BIBTEX_Abstract>Acquiring 3D models of intricate objects (like tree branches, bicycles and insects) is a challenging task due to severe self-occlusions, repeated thin structures, and surface discontinuities. In theory, a shape-from-silhouettes (SFS) approach can overcome these difficulties and reconstruct visual hulls that are close to the actual shapes, regardless of the complexity of the object. In practice, however, SFS is highly sensitive to errors in silhouette contours and the calibration of the imaging system, and has therefore not been used for obtaining accurate shapes with a large number of views. In this work, we present a practical approach to SFS using a novel technique called coplanar shadowgram imaging that allows us to use dozens to even hundreds of views for visual hull reconstruction. A point light source is moved around an object and the shadows (silhouettes) cast onto a single background plane are imaged. We characterize this imaging system in terms of image projection, reconstruction ambiguity, epipolar geometry, and shape and source recovery. The coplanarity of the shadowgrams yields unique geometric properties that are not possible in traditional multi-view camera-based imaging systems. These properties allow us to derive a robust and automatic algorithm to recover the visual hull of an object and the 3D positions of the light source simultaneously, regardless of the complexity of the object. We demonstrate the acquisition of several intricate shapes with severe occlusions and thin structures, using 50 to 120 views.</b:BIBTEX_Abstract></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>Lavoue2008</b:Tag><b:LCID>0</b:LCID><b:Title>Markov Random Fields for Improving 3D Mesh Analysis and Segmentation</b:Title><b:Year>2008</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Lavoué</b:Last><b:First>Guillaume</b:First></b:Person><b:Person><b:Last>Wolf</b:Last><b:First>Christian</b:First></b:Person></b:NameList></b:Author></b:Author><b:BookTitle>Eurographics 2008 Workshop on 3D Object Retrieval</b:BookTitle><b:BIBTEX_Abstract>Mesh analysis and clustering have became important issues in order to improve the efficiency of common processingoperations like compression, watermarking or simplification. In this context we present a new method for clustering / labeling a 3D mesh given any field of scalar values associated with its vertices (curvature, density, roughness etc.). Our algorithm is based on Markov Random Fields, graphical probabilistic models. This Bayesian framework allows (1) to integrate both the attributes and the geometry in the clustering, and (2) to obtain an optimal global solution using only local interactions, due to the Markov property of the random field. We have defined new observation and prior models for 3D meshes, adapted from image processing which achieve very good results in terms of spatial coherency of the labeling. All model parameters are estimated, resulting in a fully automatic process (the only required parameter is the number of clusters) which works in reasonable time (several seconds).</b:BIBTEX_Abstract></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>Xiang2008</b:Tag><b:Title>Skeletonization of Branched Volume by Shape Decomposition</b:Title><b:Year>2008</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Xiang</b:Last><b:First>Bo</b:First></b:Person><b:Person><b:Last>Zhang</b:Last><b:First>Xiaopeng</b:First></b:Person><b:Person><b:Last>Ma</b:Last><b:First>Wei</b:First></b:Person><b:Person><b:Last>Zha</b:Last><b:First>Hongbin</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>1-6</b:Pages><b:BookTitle>Pattern Recognition</b:BookTitle><b:JournalName>Pattern Recognition, 2008. CCPR '08. Chinese Conference on</b:JournalName><b:BIBTEX_Abstract>We present an algorithm to automatically extract skeletons for branched volumes by shape decomposition. First, a region growing strategy is adopted based on a distance transformation to decompose a volume into several meaningful components with simple topological structures. Then, the skeleton of each component is individually extracted. Finally, the skeletons of all the components are integrated and a structural skeleton of the volume data is obtained, where the structural skeleton is topologically equivalent to the volume. The contributions of the algorithm are: the elimination of the influence of different branches and the accurate skeleton extraction with topological structure of the model due to exact decomposition. Experiments show that this algorithm is applicable to shapes with complex topology.</b:BIBTEX_Abstract><b:BIBTEX_KeyWords>feature extraction, image thinning, topologybranched volume skeletonization, distance transformation, region growing strategy, shape decomposition, skeletons extraction, topological structure</b:BIBTEX_KeyWords></b:Source><b:Source><b:SourceType>JournalArticle</b:SourceType><b:Tag>Kry2006</b:Tag><b:Title>Interaction capture and synthesis</b:Title><b:Year>2006</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Kry</b:Last><b:Middle>G.</b:Middle><b:First>Paul</b:First></b:Person><b:Person><b:Last>Pai</b:Last><b:Middle>K.</b:Middle><b:First>Dinesh</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>872-880</b:Pages><b:Volume>25</b:Volume><b:Publisher>ACM Press</b:Publisher><b:JournalName>ACM Trans. Graph.</b:JournalName><b:BIBTEX_Abstract>Modifying motion capture to satisfy the constraints of new animation is difficult when contact is involved, and a critical problem for animation of hands. The compliance with which a character makes contact also reveals important aspects of the movement’s purpose. We present a new technique called interaction capture, for capturing these contact phenomena. We capture contact forces at the same time as motion, at a high rate, and use both to estimate a nominal reference trajectory and joint compliance. Unlike traditional methods, our method estimates joint compliance without the need for motorized perturbation devices. New interactions can then be synthesized by physically based simulation. We describe a novel position-based linear complementarity problem formulation that includes friction, breaking contact, and the compliant coupling between contacts at different fingers. The technique is validated using data from previous work and our own perturbation-based estimates.</b:BIBTEX_Abstract></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>Aguiar2008</b:Tag><b:Title>Performance capture from sparse multi-view video</b:Title><b:Year>2008</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Aguiar</b:Last><b:Middle>de</b:Middle><b:First>Edilson</b:First></b:Person><b:Person><b:Last>Stoll</b:Last><b:First>Carsten</b:First></b:Person><b:Person><b:Last>Theobalt</b:Last><b:First>Christian</b:First></b:Person><b:Person><b:Last>Ahmed</b:Last><b:First>Naveed</b:First></b:Person><b:Person><b:Last>Seidel</b:Last><b:First>Hans-Peter</b:First></b:Person><b:Person><b:Last>Thrun</b:Last><b:First>Sebastian</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>1-10</b:Pages><b:Publisher>ACM</b:Publisher><b:BookTitle>SIGGRAPH '08: ACM SIGGRAPH 2008 papers</b:BookTitle><b:BIBTEX_Abstract>This paper proposes a new marker-less approach to capturing human performances from multi-view video. Our algorithm can jointly reconstruct spatio-temporally coherent geometry, motion and textural surface appearance of actors that perform complex and rapid moves. Furthermore, since our algorithm is purely meshbased and makes as few as possible prior assumptions about the type of subject being tracked, it can even capture performances of people wearing wide apparel, such as a dancer wearing a skirt. To serve this purpose our method efficiently and effectively combines the power of surface- and volume-based shape deformation techniques with a new mesh-based analysis-through-synthesis framework. This framework extracts motion constraints from video and makes the laser-scan of the tracked subject mimic the recorded performance. Also small-scale time-varying shape detail is recovered by applying model-guided multi-view stereo to refine the model surface. Our method delivers captured performance data at higher level of detail, is highly versatile, and is applicable to many complex types of scenes that could not be handled by alternative marker-based or marker-free recording techniques.</b:BIBTEX_Abstract></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>Ahmed2005</b:Tag><b:Title>Automatic Generation of Personalized Human Avatars from Multi-View Video</b:Title><b:Year>2005</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Ahmed</b:Last><b:First>Naveed</b:First></b:Person><b:Person><b:Last>Aguiar</b:Last><b:Middle>de</b:Middle><b:First>Edilson</b:First></b:Person><b:Person><b:Last>Theobalt</b:Last><b:First>Christian</b:First></b:Person><b:Person><b:Last>Magnor</b:Last><b:First>Marcus</b:First></b:Person><b:Person><b:Last>Seidel</b:Last><b:First>Hans-Peter</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>257-260</b:Pages><b:Publisher>ACM</b:Publisher><b:BookTitle>VRST '05: Proceedings of the ACM symposium on Virtual reality software and technology</b:BookTitle><b:ConferenceName>Association for Computing Machinery (ACM)</b:ConferenceName><b:BIBTEX_Abstract>In multi-user virtual environments, like online games or 3D chat rooms, real-world people interact via digital avatars. In order to make the step from the real world onto the virtual stage convincing the digital equivalent of the user has to be personalized. It should reflect the shape and proportions, the kinematic properties, as well as the textural appearance of its real-world equivalent. In [1] we present a novel easy-to-use and fully-automatic approach to create a personalized avatar from multi-view video data of a moving person. An adaptable generic human body model is scaled and deformed until its shape and skeletal dimensions match the real human shown in the video footage. A consistent surface texture for the model is generated using multi-view video frames from different camera views and different body poses. With our proposed method photo-realistic human avatars can be robustly generated.</b:BIBTEX_Abstract></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>Khan2008</b:Tag><b:Title>Reconstructing non-stationary articulated objects in monocular video using silhouette information</b:Title><b:Year>2008</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Khan</b:Last><b:First>S.M.</b:First></b:Person><b:Person><b:Last>Shah</b:Last><b:First>M.</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>1-8</b:Pages><b:BookTitle>Computer Vision and Pattern Recognition</b:BookTitle><b:JournalName>Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on</b:JournalName><b:BIBTEX_Abstract>This paper presents an approach to reconstruct non-stationary, articulated objects from silhouettes obtained with a monocular video sequence. We introduce the concept of motion blurred scene occupancies, a direct analogy of motion blurred images but in a 3D object scene occupancy space resulting from the motion/deformation of the object. Our approach starts with an image based fusion step that combines color and silhouette information from multiple views. To this end we propose to use a novel construct: the temporal occupancy point (TOP), which is the estimated 3D scene location of a silhouette pixel and contains information about duration of time it is occupied. Instead of explicitly computing the TOP in 3D space we directly obtain itpsilas imaged(projected) locations in each view. This enables us to handle monocular video and arbitrary camera motion in scenarios where complete camera calibration information may not be available. The result is a set of blurred scene occupancy images in the corresponding views, where the values at each pixel correspond to the fraction of total time duration that the pixel observed an occupied scene location. We then use a motion de-blurring approach to de-blur the occupancy images. The de-blurred occupancy images correspond to a silhouettes of the mean/motion compensated object shape and are used to obtain a visual hull reconstruction of the object. We show promising results on challenging monocular datasets of deforming objects where traditional visual hull intersection approaches fail to reconstruct the object correctly.</b:BIBTEX_Abstract><b:BIBTEX_KeyWords>image motion analysis, image restoration, image sequences3D object scene occupancy space, 3D scene location, camera motion, image based fusion step, monocular video sequence, motion blurred images, motion blurred scene occupancies, motion deblurring, nonstationary articulated object reconstruction, silhouette information, silhouette pixel, temporal occupancy point</b:BIBTEX_KeyWords></b:Source><b:Source><b:SourceType>JournalArticle</b:SourceType><b:Tag>1487512</b:Tag><b:Title>Confocal Stereo</b:Title><b:Year>2009</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Hasinoff</b:Last><b:Middle>W.</b:Middle><b:First>Samuel</b:First></b:Person><b:Person><b:Last>Kutulakos</b:Last><b:Middle>N.</b:Middle><b:First>Kiriakos</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>82-104</b:Pages><b:Volume>81</b:Volume><b:Publisher>Kluwer Academic Publishers</b:Publisher><b:JournalName>Int. J. Comput. Vision</b:JournalName><b:BIBTEX_Abstract>We present confocal stereo, a new method for computing 3D shape by controlling the focus and aperture of a lens. The method is specifically designed for reconstructing scenes with high geometric complexity or fine-scale texture. To achieve this, we introduce the confocal constancy property, which states that as the lens aperture varies, the pixel intensity of a visible in-focus scene point will vary in a scene-independent way, that can be predicted by prior radiometric lens calibration. The only requirement is that incoming radiance within the cone subtended by the largest aperture is nearly constant. First, we develop a detailed lens model that factors out the distortions in high resolution SLR cameras (12MP or more) with large-aperture lenses (e.g., f1.2). This allows us to assemble an A×F aperture-focus image (AFI) for each pixel, that collects the undistorted measurements over all A apertures and F focus settings. In the AFI representation, confocal constancy reduces to color comparisons within regions of the AFI, and leads to focus metrics that can be evaluated separately for each pixel. We propose two such metrics and present initial reconstruction results for complex scenes, as well as for a scene with known ground-truth shape.</b:BIBTEX_Abstract></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>Ladikos2008</b:Tag><b:Title>Efficient visual hull computation for real-time 3D reconstruction using CUDA</b:Title><b:Year>2008</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Ladikos</b:Last><b:First>A.</b:First></b:Person><b:Person><b:Last>Benhimane</b:Last><b:First>S.</b:First></b:Person><b:Person><b:Last>Navab</b:Last><b:First>N.</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>1-8</b:Pages><b:BookTitle>Computer Vision and Pattern Recognition Workshops</b:BookTitle><b:JournalName>Computer Vision and Pattern Recognition Workshops, 2008. CVPRW '08. IEEE Computer Society Conference on</b:JournalName><b:BIBTEX_Abstract>In this paper we present two efficient GPU-based visual hull computation algorithms. We compare them in terms of performance using image sets of varying size and different voxel resolutions. In addition, we present a real-time 3D reconstruction system which uses the proposed GPU-based reconstruction method to achieve real-time performance (30 fps) using 16 cameras and 4 PCs.</b:BIBTEX_Abstract><b:BIBTEX_KeyWords>computational geometry, image reconstruction, image resolutionCUDA, GPU, image sets, real-time 3D reconstruction, real-time performance, visual hull computation, voxel resolutions</b:BIBTEX_KeyWords></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>Remondino2002</b:Tag><b:Title>Human Body Reconstruction from Image Sequences</b:Title><b:Year>2002</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Remondino</b:Last><b:First>Fabio</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>50-57</b:Pages><b:Publisher>Springer</b:Publisher><b:BookTitle>Pattern Recognition (DAGM 2002), Lecture Notes in Computer Science 2449</b:BookTitle><b:BIBTEX_Abstract>The generation of 3-D models from uncalibrated sequences is a challenging problem that has been investigated in many research activities in the last decade. In particular, a topic of great interest is the modeling of real humans. In this paper a method for the 3-D reconstruction of static human body shapes from images acquired with a video-camera is presented. The process includes the orientation and calibration of the sequence, the extraction of correspondences on the body using least squares matching technique and the reconstruction of the 3-D point cloud of the human body.</b:BIBTEX_Abstract></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>Guan2006</b:Tag><b:Title>Visual Hull Construction in the Presence of Partial Occlusion</b:Title><b:Year>2006</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Guan</b:Last><b:First>Li</b:First></b:Person><b:Person><b:Last>Sinha</b:Last><b:First>S.</b:First></b:Person><b:Person><b:Last>Franco</b:Last><b:First>J.-S.</b:First></b:Person><b:Person><b:Last>Pollefeys</b:Last><b:First>M.</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>413-420</b:Pages><b:BookTitle>computational complexity</b:BookTitle><b:JournalName>3D Data Processing, Visualization, and Transmission, Third International Symposium on</b:JournalName><b:BIBTEX_Abstract>In this paper, we propose a visual hull algorithm, which guarantees a correct construction even in the presence of partial occlusion, while "correct" here means that the real shape is located inside the visual hull. The algorithm is based on a new idea of the "extended silhouette", which requires the silhouette from background subtraction and the "occlusion mask" of the same view. In order to prepare the occlusion mask, we also propose a novel concept of "effective boundary" of moving foreground objects in a video obtained from a static camera. The accumulation of the effective boundary through time automatically gives robust occluder boundaries. We theoretically prove that our algorithm deterministically computes the tightest, correct visual hull in the presence of occlusion. Both synthetic and real examples are given as a demonstration of the correctness of the algorithm. Finally we analyze that this new algorithm is still within the time complexity of the traditional method.</b:BIBTEX_Abstract><b:BIBTEX_KeyWords>computational complexity, image denoisingbackground subtraction, extended silhouette, occlusion mask, partial occlusion, time complexity, visual hull construction</b:BIBTEX_KeyWords></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>Laurentini2003</b:Tag><b:Title>The visual hull for understanding shapes from contours: a survey</b:Title><b:Year>2003</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Laurentini</b:Last><b:First>A.</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages> 25-28 vol.1</b:Pages><b:Volume>1</b:Volume><b:BookTitle>Signal Processing and Its Applications</b:BookTitle><b:JournalName>Signal Processing and Its Applications, 2003. Proceedings. Seventh International Symposium on</b:JournalName><b:BIBTEX_Abstract> In several practical situations the only available information for recognizing or reconstructing 3D objects are the object contours. Some recent theoretical developments put on a firm ground understanding 3D shapes from silhouettes. In this paper we present survey of these developments, related to the geometric concept of visual hull.</b:BIBTEX_Abstract><b:BIBTEX_KeyWords> image recognition, image reconstruction, object recognition 3D object recognizing, 3D object reconstructing, 3D shape, object contour, silhouette, visual hull geometric concept</b:BIBTEX_KeyWords></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>Min2008</b:Tag><b:Title>Graph-Cut Based Background Subtraction Using Visual Hull in Multiveiw Images</b:Title><b:Year>2008</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Min</b:Last><b:First>Seungki</b:First></b:Person><b:Person><b:Last>Kim</b:Last><b:First>Jungwhan</b:First></b:Person><b:Person><b:Last>Park</b:Last><b:First>Anjin</b:First></b:Person><b:Person><b:Last>Hong</b:Last><b:First>Gwangjin</b:First></b:Person><b:Person><b:Last>Jung</b:Last><b:First>Keechul</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>372-377</b:Pages><b:BookTitle>Computing: Techniques and Applications</b:BookTitle><b:JournalName>Computing: Techniques and Applications, 2008. DICTA '08.Digital Image</b:JournalName><b:BIBTEX_Abstract>A graph-cut method has been successfully used in many applications for image segmentation. However, it needs lots of time and user intervention. In case of multi view image (MVI), it is especially hard to segment all images in a short time because of numerous images in MVI. In this paper, we describe a new technique for multi view image segmentation, which needs minimum user intervention and provides fast processing time. The user marks certain pixels as "target object" or "background" to provide a constraint for segmentation to only one of the MVI. The seed information is propagated to all images in the MVI. In this step, we can acquire tentative segment result and then apply them to reconstruct the 3D model which exploits the visual hull. After the 3D model is reconstructed, segment error that is found located out of foreground is eliminated. Although visual hull has a shortcoming that cannot represent whether the object is convex or concave, tentative segment result is easy to use and proven to be enough as our proposed method. We can acquire final segment result in a short time by integrating these two simple methods. According to the experiments, our method shows better performance in terms of processing time and minimizing user intervention.</b:BIBTEX_Abstract><b:BIBTEX_KeyWords>graph theory, image reconstruction, image representation, image segmentation3D model reconstruction, graph-cut method, image segmentation, multiview image segmentation, target object, visual hull</b:BIBTEX_KeyWords></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>Magnor2004</b:Tag><b:Title>Spacetime-Coherent Geometry Reconstruction from Multiple Video Streams</b:Title><b:Year>2004</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Magnor</b:Last><b:First>Marcus</b:First></b:Person><b:Person><b:Last>Goldlucke</b:Last><b:First>Bastian</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>365-372</b:Pages><b:Publisher>IEEE Computer Society</b:Publisher><b:BookTitle>3DPVT '04: Proceedings of the 3D Data Processing, Visualization, and Transmission, 2nd International Symposium</b:BookTitle><b:BIBTEX_Abstract>By reconstructing time-varying geometry one frame at a time, one ignores the continuity of natural motion, wasting useful information about the underlying video-image formation process and taking into account temporally discontinuous reconstruction results. In 4D spacetime, the surface of a dynamic object describes a continuous 3D hyper-surface. This hyper-surface can be implicitly defined as the minimum of an energy functional designed to optimize photo-consistency. Based on an Euler-Lagrange reformulation of the problem, we find this hyper-surface from a handful of synchronized video recordings. The resulting object geometry varies smoothly over time, and intermittently invisible object regions are correctly interpolated from previously and/or future frames.</b:BIBTEX_Abstract></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>Kry2009</b:Tag><b:Title>Modal locomotion: animating virtual characters with natural vibrations</b:Title><b:Year>2009</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Kry</b:Last><b:First>Paul</b:First></b:Person><b:Person><b:Last>Rev\'eret</b:Last><b:First>Lionel</b:First></b:Person><b:Person><b:Last>Faure</b:Last><b:First>Fran\c{c}ois</b:First></b:Person><b:Person><b:Last>Cani</b:Last><b:First>Marie-Paule</b:First></b:Person></b:NameList></b:Author></b:Author><b:BookTitle>Eurographics, , 2009</b:BookTitle><b:BIBTEX_Abstract>We present a general method to intuitively create a wide range of locomotion controllers for 3D legged characters. The key of our approach is the assumption that efficient locomotion can exploit the natural vibration modes of the body, where these modes are related to morphological parameters such as the shape, size, mass, and joint stiffness. The vibration modes are computed for a mechanical model of any 3D character with rigid bones, elastic joints, and additional constraints as desired. A small number of vibration modes can be selected with respect to their relevance to locomotion patterns and combined into a compact controller driven by very few parameters. We show that these controllers can be used in dynamic simulations of simple creatures, and for kinematic animations of more complex creatures of a variety of shapes and sizes.</b:BIBTEX_Abstract></b:Source><b:Source><b:SourceType>JournalArticle</b:SourceType><b:Tag>Slabaugh2004</b:Tag><b:Title>Methods for Volumetric Reconstruction of Visual Scenes</b:Title><b:Year>2004</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Slabaugh</b:Last><b:Middle>G.</b:Middle><b:First>Gregory</b:First></b:Person><b:Person><b:Last>Culbertson</b:Last><b:Middle>Bruce</b:Middle><b:First>W.</b:First></b:Person><b:Person><b:Last>Malzbender</b:Last><b:First>Thomas</b:First></b:Person><b:Person><b:Last>Stevens</b:Last><b:Middle>R.</b:Middle><b:First>Mark</b:First></b:Person><b:Person><b:Last>Schafer</b:Last><b:Middle>W.</b:Middle><b:First>Ronald</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>179-199</b:Pages><b:Volume>57</b:Volume><b:JournalName>International Journal of Computer Vision</b:JournalName><b:BIBTEX_Abstract>In this paper, we present methods for 3D volumetric reconstruction of visual scenes photographed by multiple calibrated cameras placed at arbitrary viewpoints. Our goal is to generate a 3D model that can be rendered to synthesize new photo-realistic views of the scene. We improve upon existing voxel coloring/space carving approaches by introducing new ways to compute visibility and photo-consistency, as well as model infinitely large scenes. In particular, we describe a visibility approach that uses all possible color information from the photographs during reconstruction, photo-consistency measures that are more robust and/or require less manual intervention, and a volumetric warping method for application of these reconstruction methods to large-scale scenes.</b:BIBTEX_Abstract></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>Hachet2005</b:Tag><b:Title>A Camera-Based Interface for Interaction with Mobile Handheld Computers</b:Title><b:Year>2005</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Hachet</b:Last><b:First>Martin</b:First></b:Person><b:Person><b:Last>Pouderoux</b:Last><b:First>Joachim</b:First></b:Person><b:Person><b:Last>Guitton</b:Last><b:First>Pascal</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>65-71</b:Pages><b:Publisher>ACM Press</b:Publisher><b:BookTitle>Proceedings of I3D'05 - ACM SIGGRAPH 2005 Symposium on Interactive 3D Graphics and Games</b:BookTitle><b:BIBTEX_Abstract>Recent advances in mobile computing allow the users to deal with 3D interactive graphics on handheld computers. Although the computing resources and screen resolutions grow steadily, user interfaces for handheld computers do not change significantly. Consequently, we designed a new 3-DOF interface adapted to the characteristics of handheld computers. This interface tracks the movement of a target that the user holds behind the screen by analyzing the video stream of the handheld computer camera. The position of the target is directly inferred from the color-codes that are printed on it using an efficient algorithm. The users can easily interact in real-time in a mobile setting. The visualization of the data is good as the target does not occlude the screen and the interaction techniques are not dependent on the orientation of the handheld computer. We used the interface in several test applications for the visualization of large images such as maps, the manipulation of 3D models, and the navigation in 3D scenes. This new interface favors the development of 2D and 3D interactive applications on handheld computers.</b:BIBTEX_Abstract><b:BIBTEX_KeyWords>User interfaces, Interaction, PDA</b:BIBTEX_KeyWords></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>Cheng2006</b:Tag><b:Title>Tree Skeleton Extraction from a Single Range Image</b:Title><b:Year>2006</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Cheng</b:Last><b:First>Zhanglin</b:First></b:Person><b:Person><b:Last>Zhang</b:Last><b:First>Xiaopeng</b:First></b:Person><b:Person><b:Last>Fourcaud</b:Last><b:First>Thierry</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>274-281</b:Pages><b:Publisher>IEEE Computer Society</b:Publisher><b:BookTitle>PMA '06: Proceedings of the 2006 International Symposium on Plant Growth Modeling, Simulation, Visualization and Applications</b:BookTitle><b:BIBTEX_Abstract>Tree skeleton computation is of significance in the ge- ometric modeling of botanic trees and in the application of forestry. This paper describes an approach to extract branch skeletons from a single range image of a tree. A ba- sis of this approach is that the trunk and branches are mod- eled by generalized circular cylinders, and the tree skeleton is defined as the connected curve-axes of these cylinders. In the proposed system, the range image is partitioned into patches at first based on the discontinuity of depth and axis direction, where each patch only contains points from the same branch. Then each patch is fitted with a series of cir- cular cylinders. Finally the tree skeleton is generated by se- quentially connecting the skeleton points of fitted cylinders. This work shows that cylinder fitting can be used to han- dle the incompleteness of input data, and generate accurate skeleton points and corresponding radii. The main contri- bution of this paper is that we proposed a new definition and computation of tree skeletons and introduced an effi- cient and robust cylinder fitting method. Experiment shows the effectiveness of this approach.</b:BIBTEX_Abstract></b:Source><b:Source><b:SourceType>JournalArticle</b:SourceType><b:Tag>Wang2008</b:Tag><b:Title>Curve-Skeleton Extraction Using Iterative Least Squares Optimization</b:Title><b:Year>2008</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Wang</b:Last><b:First>Yu-Shuen</b:First></b:Person><b:Person><b:Last>Lee</b:Last><b:First>Tong-Yee</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>926-936</b:Pages><b:Volume>14</b:Volume><b:Publisher>IEEE Educational Activities Department</b:Publisher><b:JournalName>IEEE Transactions on Visualization and Computer Graphics</b:JournalName><b:BIBTEX_Abstract>A curve skeleton is a compact representation of 3D objects and has numerous applications. It can be used to describe an object¡¦s geometry and topology. In this paper, we introduce a novel approach for computing curve skeletons for volumetric representations of the input models. Our algorithm consists of three major steps: 1) using iterative least squares optimization to shrink models and, at the same time, preserving their geometries and topologies; 2) extracting curve skeletons through the thinning algorithm; and 3) pruning unnecessary branches based on shrinking ratios. The proposed method is less sensitive to noise on the surface of models and can generate smoother skeletons. In addition, our shrinking algorithm requires little computation, since the optimization system can be factorized and stored in the pre-computational step. We demonstrate several extracted skeletons that help evaluate our algorithm. We also experimentally compare the proposed method with other well-known methods. Experimental results show advantages when using our method over other techniques.</b:BIBTEX_Abstract></b:Source><b:Source><b:SourceType>JournalArticle</b:SourceType><b:Tag>Shin2008</b:Tag><b:Title>Local Hull-Based Surface Construction of Volumetric Data From Silhouettes</b:Title><b:Year>2008</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Shin</b:Last><b:First>Dongjoe</b:First></b:Person><b:Person><b:Last>Tjahjadi</b:Last><b:First>T.</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>1251-1260</b:Pages><b:Volume>17</b:Volume><b:JournalName>Image Processing, IEEE Transactions on</b:JournalName><b:BIBTEX_Abstract>The marching cubes (MC) is a general method which can construct a surface of an object from its volumetric data generated using a shape from silhouette method. Although MC is efficient and straightforward to implement, a MC surface may have discontinuity even though the volumetric data is continuous. This is because surface construction is more sensitive to image noise than the construction of volumetric data. To address this problem, we propose a surface construction algorithm which aggregates local surfaces constructed by the 3-D convex hull algorithm. Thus, the proposed method initially classifies local convexities from imperfect MC vertices based on sliced volumetric data. Experimental results show that continuous surfaces are obtained from imperfect silhouette images of both convex and nonconvex objects.</b:BIBTEX_Abstract><b:BIBTEX_KeyWords>image denoising, image reconstruction3D convex hull algorithm, image noise, local hull-based surface construction, marching cubes, silhouette method, volumetric data</b:BIBTEX_KeyWords></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>Park2002a</b:Tag><b:Title>Shape Decomposition and Skeleton Extraction of Character Patterns</b:Title><b:Year>2002</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Park</b:Last><b:First>Jeong-Sun</b:First></b:Person><b:Person><b:Last>Oh</b:Last><b:First>Il-Seok</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>30411</b:Pages><b:Publisher>IEEE Computer Society</b:Publisher><b:BookTitle>ICPR '02: Proceedings of the 16 th International Conference on Pattern Recognition (ICPR'02) Volume 3</b:BookTitle><b:BIBTEX_Abstract>This paper proposes an approach to extract skeletons from the character patterns. It first decomposes the pattern into a set of near-convex parts and then extracts skeletons from the parts. In shape decomposition stage, the convex hull information is used to identify the splitting paths. For the skeleton extraction, an operation that ties the adjacent strokes by a knot is developed. Our control procedure processes a variety of different situations of the adjacentstrokes in a systematic way.</b:BIBTEX_Abstract></b:Source><b:Source><b:SourceType>JournalArticle</b:SourceType><b:Tag>Turaga2009</b:Tag><b:Title>Unsupervised view and rate invariant clustering of video sequences</b:Title><b:Year>2009</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Turaga</b:Last><b:First>Pavan</b:First></b:Person><b:Person><b:Last>Veeraraghavan</b:Last><b:First>Ashok</b:First></b:Person><b:Person><b:Last>Chellappa</b:Last><b:First>Rama</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>353-371</b:Pages><b:Volume>113</b:Volume><b:Publisher>Elsevier Science Inc.</b:Publisher><b:JournalName>Comput. Vis. Image Underst.</b:JournalName><b:BIBTEX_Abstract>Videos play an ever increasing role in our everyday lives with applications ranging from news, entertainment, scientific research, security and surveillance. Coupled with the fact that cameras and storage media are becoming less expensive, it has resulted in people producing more video content than ever before. This necessitates the development of efficient indexing and retrieval algorithms for video data. Most state-of-the-art techniques index videos according to the global content in the scene such as color, texture, brightness, etc. In this paper, we discuss the problem of activity-based indexing of videos. To address the problem, first we describe activities as a cascade of dynamical systems which significantly enhances the expressive power of the model while retaining many of the computational advantages of using dynamical models. Second, we also derive methods to incorporate view and rate-invariance into these models so that similar actions are clustered together irrespective of the viewpoint or the rate of execution of the activity. We also derive algorithms to learn the model parameters from a video stream and demonstrate how a single video sequence may be clustered into different clusters where each cluster represents an activity. Experimental results for five different databases show that the clusters found by the algorithm correspond to semantically meaningful activities.</b:BIBTEX_Abstract></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>Li2007</b:Tag><b:Title>3D object recognition from range images using pyramid matching</b:Title><b:Year>2007</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Li</b:Last><b:First>Xinju</b:First></b:Person><b:Person><b:Last>Guskov</b:Last><b:First>I.</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>1-6</b:Pages><b:BookTitle>Computer Vision, 2007. ICCV 2007. IEEE 11th International Conference on</b:BookTitle><b:JournalName>Computer Vision, 2007. ICCV 2007. IEEE 11th International Conference on</b:JournalName><b:BIBTEX_Abstract>Recognition of 3D objects from different viewpoints is a difficult problem. In this paper, we propose a new method to recognize 3D range images by matching local surface descriptors. The input 3D surfaces are first converted into a set of local shape descriptors computed on surface patches defined by detected salient features. We compute the similarities between input 3D images by matching their descriptors with a pyramid kernel function. The similarity matrix of the images is used to train for classification using SVM, and new images can be recognized by comparing with the training set. The approach is evaluated on both synthetic and real 3D data with complex shapes.</b:BIBTEX_Abstract><b:BIBTEX_KeyWords>image matching, learning (artificial intelligence), object recognition, support vector machines3D object recognition, 3D range images, SVM, local shape descriptors, pyramid kernel function, training set</b:BIBTEX_KeyWords></b:Source><b:Source><b:SourceType>JournalArticle</b:SourceType><b:Tag>Tierny2008a</b:Tag><b:Title>Enhancing 3D mesh topological skeletons with discrete contour constrictions</b:Title><b:Year>2008</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Tierny</b:Last><b:First>Julien</b:First></b:Person><b:Person><b:Last>Vandeborre</b:Last><b:First>Jean-Philippe</b:First></b:Person><b:Person><b:Last>Daoudi</b:Last><b:First>Mohamed</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>155-172</b:Pages><b:Volume>24</b:Volume><b:JournalName>The Visual Computer</b:JournalName><b:BIBTEX_Abstract>Abstract&amp;nbsp;&amp;nbsp;This paper describes a unified and fully automatic algorithm for Reeb graph construction and simplification as well as constriction approximation on triangulated surfaces.</b:BIBTEX_Abstract></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>Jia2008</b:Tag><b:Title>Markerless human body motion capture using multiple cameras</b:Title><b:Year>2008</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Jia</b:Last><b:First>Li</b:First></b:Person><b:Person><b:Last>Zhenjiang</b:Last><b:First>Miao</b:First></b:Person><b:Person><b:Last>Chengkai</b:Last><b:First>Wan</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>1469-1474</b:Pages><b:BookTitle>Signal Processing</b:BookTitle><b:JournalName>Signal Processing, 2008. ICSP 2008. 9th International Conference on</b:JournalName><b:BIBTEX_Abstract>In this paper, we present an approach for markerless model-based full human-body motion capture using multi-view images as input. We extract volume data (voxels) representation from the silhouettes extracted from multiple-view video images by the method of shape from Silhouettes (SFS), and match our predefined human body model to the volume data. We construct an energy field in the volume of interest based on the volume data and human body model with pose parameters, and transform the matching to an energy minimizing problem. By dynamic graph cut, we get the minimum energy of certain pose parameters, and at last we optimize the pose parameters using Powell algorithm with a novel approach that uses the linear prediction guiding the optimization process and get the pose recovered. Through the test results on several video sequences of human body movements in an unaugmented office environment, we demonstrate the effectiveness and robustness of our approach.</b:BIBTEX_Abstract><b:BIBTEX_KeyWords>cameras, image motion analysis, image sequences, optimisationPowell algorithm, dynamic graph cut, linear prediction, markerless human body motion, multiple cameras, multiple-view video images, optimization process, pose parameter, unaugmented office environment, video sequences, volume data representation</b:BIBTEX_KeyWords></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>AitAider2006</b:Tag><b:Title>Simultaneous Object Pose and Velocity Computation Using a Single View from a Rolling Shutter Camera</b:Title><b:Year>2006</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Aider</b:Last><b:Middle>Ait</b:Middle><b:First>O.</b:First></b:Person><b:Person><b:Last>Andreff</b:Last><b:First>N.</b:First></b:Person><b:Person><b:Last>Lavest</b:Last><b:First>J.M.</b:First></b:Person><b:Person><b:Last>Martinet</b:Last><b:First>P.</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>II: 56-68</b:Pages><b:BookTitle>#ECCV06#</b:BookTitle><b:BIBTEX_Abstract>An original method for computing instantaneous 3D pose and velocity of fast moving objects using a single view is presented. It exploits image deformations induced by rolling shutter in CMOS image sensors. First of all, a general perspective projection model of a moving 3D point is presented. A solution for the pose and velocity recovery problem is then described. The method is based on bundle adjustment and uses point correspondences. The resulting algorithm enables to transform a CMOS low cost and low power camera into an original velocity sensor. Finally, experimental results with real data confirm the relevance of the approach.</b:BIBTEX_Abstract></b:Source><b:Source><b:SourceType>JournalArticle</b:SourceType><b:Tag>Lanman2008</b:Tag><b:Title>Shield Fields: Modeling and Capturing 3D Occluders</b:Title><b:Year>2008</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Lanman</b:Last><b:First>Douglas</b:First></b:Person><b:Person><b:Last>Raskar</b:Last><b:First>Ramesh</b:First></b:Person><b:Person><b:Last>Agrawal</b:Last><b:First>Amit</b:First></b:Person><b:Person><b:Last>Taubin</b:Last><b:First>Gabriel</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>10</b:Pages><b:Volume>27</b:Volume><b:JournalName>ACM Transactions on Graphics (Proc. SIGGRAPH Asia)</b:JournalName><b:BIBTEX_Abstract>We describe a unified representation of occluders in light transport and photography using shield fields: the 4D attenuation function which acts on any light field incident on an occluder. Our key theoretical result is that shield fields can be used to decouple the effects of occluders from the incident illumination. We first describe the properties of shield fields in the frequency-domain and briefly analyze the “forward” problem of efficiently computing cast shadows. Afterwards, we apply the shield field signal-processing framework to make several new observations regarding the “inverse” problem of reconstructing 3D occluders from cast shadows – extending previous work on shape-from-silhouette and visual hull methods. From this analysis we develop the first single-camera, single-shot approach to capture visual hulls without requiring moving or programmable illumination. We analyze several competing camera designs – ultimately leading to the development of a new large-format, mask-based light field camera that exploits optimal tiled-broadband codes for light-efficient shield field capture. We conclude by presenting a detailed experimental analysis of shield field capture and 3D occluder reconstruction.</b:BIBTEX_Abstract></b:Source><b:Source><b:SourceType>Report</b:SourceType><b:BIBTEX_Entry>phdthesis</b:BIBTEX_Entry><b:Tag>Campos2006</b:Tag><b:Title>3D Visual Tracking of Articulated Objects and Hands</b:Title><b:Year>2006</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>de</b:Last><b:Middle>Em?dio</b:Middle><b:First>Teo?lo</b:First></b:Person></b:NameList></b:Author></b:Author><b:Department>Department of Engineering Science - University of Oxford</b:Department><b:BIBTEX_Abstract>The ability to track multiple and articulated objects is an important one, not least in the areas of autonomous and teleoperated robotics, visual surveillance and human motion analysis. This thesis is concerned with marker-free real-time detection and tracking of articulated objects, targeting human hands with the aim to study methods that can be applied to enhance the interaction between humans and 3D (real or virtual) objects.

A survey summarises methods used to approach this and related problems in the literature. It indicates that, despite the large body of research in this field over twenty or so years, the area still proves challenging. Two main approaches have been identified. The first, known as generative tracking, uses an explicit kinematical representation of linkages or constraints between object parts and tracks by minimising error of projected control points. The second, known as discriminative approach, little is specified beforehand, but training data is used in order to create a map between image observations and 3D poses. This thesis describes novel work in both areas.

In the generative area, a method for tracking of articulated objects is described. It is a new extension of a method for tracking rigid objects in which the motion constraints between parts of the object are imposed up-front within the tracking process. The inter-frame pose update is derived as the solution of a linear system. This method has been applied to track articulated objects, including hands and multiple objects with motion constraints.

An alternative method is that based on estimating the motion of each subpart independently, thereby introducing redundant degrees of freedom, and imposing constraints later in a lower dimensional subspace. This method is reviewed and a comparison between this and the aforementioned method is presented in terms of accuracy, efficiency and robustness.

In the discriminative area, an inference-based approach is adopted in which a non-parametric relation between global image measurements and 3D poses is learnt using a multivariate regressor based on Relevance Vector Machine. This relation is a continuous map that allows fast and efficient pose estimation from static images. This method can detect and estimate the 3D pose of hands from static images, so it can be applied to (re-)initialise the generative tracker.

In this thesis, the use of multiple view is adopted as a solution to reduce the ambiguities for both generative and discriminative methods. Experiments with single and multiple views are described and a novel extension of the discriminative method for multiple views is proposed and evaluated.</b:BIBTEX_Abstract></b:Source><b:Source><b:SourceType>JournalArticle</b:SourceType><b:Tag>Lazebnik2007</b:Tag><b:Title>Projective Visual Hulls</b:Title><b:Year>2007</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Lazebnik</b:Last><b:First>Svetlana</b:First></b:Person><b:Person><b:Last>Furukawa</b:Last><b:First>Yasutaka</b:First></b:Person><b:Person><b:Last>Ponce</b:Last><b:First>Jean</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>137-165</b:Pages><b:Volume>74</b:Volume><b:JournalName>International Journal of Computer Vision</b:JournalName><b:BIBTEX_Abstract>Abstract&amp;nbsp;&amp;nbsp;This article presents a novel method for computing the visual hull of a solid bounded by a smooth surface and observed by a finite set of cameras. The visual hull is the intersection of the visual cones formed by back-projecting the silhouettes found in the corresponding images. We characterize its surface as a generalized polyhedron whose faces are visual cone patches; edges are intersection curves between two viewing cones; and vertices are frontier points where the intersection of two cones is singular, or intersection points where triples of cones meet. We use the mathematical framework of oriented projective differential geometry to develop an image-based algorithm for computing the visual hull. This algorithm works in a weakly calibrated settingâ€“-that is, it only requires projective camera matrices or, equivalently, fundamental matrices for each pair of cameras. The promise of the proposed algorithm is demonstrated with experiments on several challenging data sets and a comparison to another state-of-the-art method.</b:BIBTEX_Abstract></b:Source><b:Source><b:SourceType>JournalArticle</b:SourceType><b:Tag>Remondino2004</b:Tag><b:Title>3-D Reconstruction of Static Human Body Shape from Image Sequence</b:Title><b:Year>2004</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Remondino</b:Last><b:First>Fabio</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>65-85</b:Pages><b:Volume>93</b:Volume><b:JournalName>Computer Vision and Image Understanding</b:JournalName><b:BIBTEX_Abstract>The generation of 3-D models from uncalibrated image sequences is a challenging problem that has been investigated in many research activities in the last decade. In particular, a topic of great interest is the modeling of realistic humans, for animation, manufacture or medicine purposes. Nowadays the common approaches try to reconstruct the human body using specialized hardware (laser scanners) resulting in high costs. In this contribution a different method for the three-dimensional reconstruction of static human body shape from monocular image sequence is presented. The core of the presented work describes the calibration and orientation of the images, mostly based on photogrammetric techniques. Then the process includes also the extraction of correspondences on the body using a least squares matching algorithm and the reconstruction of the 3-D body model in point cloud form.</b:BIBTEX_Abstract></b:Source><b:Source><b:SourceType>JournalArticle</b:SourceType><b:Tag>Vlasic2008</b:Tag><b:Title>Articulated mesh animation from multi-view silhouettes</b:Title><b:Year>2008</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Vlasic</b:Last><b:First>Daniel</b:First></b:Person><b:Person><b:Last>Baran</b:Last><b:First>Ilya</b:First></b:Person><b:Person><b:Last>Matusik</b:Last><b:First>Wojciech</b:First></b:Person><b:Person><b:Last>Popovi\'{c}</b:Last><b:First>Jovan</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>1-9</b:Pages><b:Volume>27</b:Volume><b:Publisher>ACM</b:Publisher><b:JournalName>ACM Trans. Graph.</b:JournalName><b:BIBTEX_Abstract>Details in mesh animations are difficult to generate but they have great impact on visual quality. In this work, we demonstrate a practical software system for capturing such details from multi-view video recordings. Given a stream of synchronized video images that record a human performance from multiple viewpoints and an articulated template of the performer, our system captures the motion of both the skeleton and the shape. The output mesh animation is enhanced with the details observed in the image silhouettes. For example, a performance in casual loose-fitting clothes will generate mesh animations with flowing garment motions. We accomplish this with a fast pose tracking method followed by nonrigid deformation of the template to fit the silhouettes. The entire process takes less than sixteen seconds per frame and requires no markers or texture cues. Captured meshes are in full correspondence making them readily usable for editing operations including texturing, deformation transfer, and deformation model learning.</b:BIBTEX_Abstract></b:Source><b:Source><b:SourceType>JournalArticle</b:SourceType><b:Tag>Chai2005</b:Tag><b:Title>Performance animation from low-dimensional control signals</b:Title><b:Year>2005</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Chai</b:Last><b:First>Jinxiang</b:First></b:Person><b:Person><b:Last>Hodgins</b:Last><b:Middle>K.</b:Middle><b:First>Jessica</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>686-696</b:Pages><b:Volume>24</b:Volume><b:Publisher>ACM</b:Publisher><b:JournalName>ACM Trans. Graph.</b:JournalName><b:BIBTEX_Abstract>This paper introduces an approach to performance animation that employs video cameras and a small set of retro-reflective markers to create a low-cost, easy-to-use system that might someday be practical for home use. The low-dimensional control signals from the user's performance are supplemented by a database of pre-recorded human motion. At run time, the system automatically learns a series of local models from a set of motion capture examples that are a close match to the marker locations captured by the cameras. These local models are then used to reconstruct the motion of the user as a full-body animation. We demonstrate the power of this approach with real-time control of six different behaviors using two video cameras and a small set of retro-reflective markers. We compare the resulting animation to animation from commercial motion capture equipment with a full set of markers.</b:BIBTEX_Abstract></b:Source><b:Source><b:SourceType>Report</b:SourceType><b:BIBTEX_Entry>phdthesis</b:BIBTEX_Entry><b:Tag>Cheung2003b</b:Tag><b:Title>Visual Hull Construction, Alignment and Refinement for Human Kinematic Modeling, Motion Tracking and Rendering</b:Title><b:Year>2003</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Cheung</b:Last><b:Middle>Man</b:Middle><b:First>Kong</b:First></b:Person></b:NameList></b:Author></b:Author><b:Department>Robotics Institute, Carnegie Mellon University</b:Department><b:BIBTEX_Abstract>The abilities to build precise human kinematic models and to perform accurate human motion tracking are essential in a wide variety of applications. Due to the complexity of the human bodies and the problem of self-occlusion, modeling and tracking humans using cameras are challenging tasks. In this thesis, we develop algorithms to perform these two tasks based on the shape estimation method Shape-From-Silhouette (SFS) which constructs a shape estimate (known as Visual Hull) of an object using its silhouettes images.

In the first half of this thesis we extend the traditional SFS algorithm so that it can be used effectively for the human related applications. To perform SFS in real-time, we propose a fast testing/projection algorithm for voxel-based SFS algorithms. Moreover, we combine silhouette information over time to effectively increase the number of cameras (and hence reconstruction details) for SFS without physically adding new cameras. We first propose a new Visual Hull representation called Bounding Edges. We then analyze the ambiguity problem of aligning two Visual Hulls. Based on the analysis, we develop an algorithm to align Visual Hulls over time using stereo and an important property of the Shape-From-Silhouette principle. This temporal SFS algorithm combines both geometric constraints and photometric consistency to align Colored Surface Points of the object extracted from the silhouette and color images. Once the Visual Hulls are aligned, they are refined by compensating for the motion of the object. The algorithm is developed for both rigid and articulated objects.

In the second half of this thesis we show how the improved SFS algorithms are used to perform the tasks of human modeling and motion tracking. First we build a system to acquire human kinematic models consisting of precise shape and joint locations. Once the kinematic models are built, they are used to track the motion of the person in new video sequences. The tracking algorithm is based on the Visual Hull alignment idea used in the temporal SFS algorithms. Finally we demonstrate how the kinematic model and the tracked motion data can be used for image-based rendering and motion transfer between two people.</b:BIBTEX_Abstract></b:Source><b:Source><b:SourceType>JournalArticle</b:SourceType><b:Tag>4178157</b:Tag><b:Title>Surface Capture for Performance-Based Animation</b:Title><b:Year>2007</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Starck</b:Last><b:First>Jonathan</b:First></b:Person><b:Person><b:Last>Hilton</b:Last><b:First>Adrian</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>21-31</b:Pages><b:Volume>27</b:Volume><b:JournalName>Computer Graphics and Applications, IEEE</b:JournalName><b:BIBTEX_Abstract>Creating realistic animated models of people is a central task in digital content production. Traditionally, highly skilled artists and animators construct shape and appearance models for digital character. They then define the character's motion at each time frame or specific key-frames in a motion sequence to create a digital performance. Increasingly, producers are using motion capture technology to record animations from an actor's performance. This technology reduces animation production time and captures natural movements to create a more believable production. However, motion capture requires the use of specialist suits and markers and only records skeletal motion. It lacks the detailed secondary surface dynamics of cloth and hair that provide the visual realism of a live performance. Over the last decade, we have investigated studio capture technology with the objective of creating models of real people that accurately reflect the time-varying shape and appearance of the whole body with clothing. Surface capture is a fully automated system for capturing a human's shape and appearance as well as motion from multiple video cameras to create highly realistic animated content from an actor's performance in full wardrobe. Our system solves two key problems in performance capture: scene capture from a limited number of camera views and efficient scene representation for visualization</b:BIBTEX_Abstract><b:BIBTEX_KeyWords>computer animation, data visualisation, image motion analysis, image sequences, realistic images, video camerasdata visualization, digital content production, motion sequence, multiple video cameras, realistic animated model, skeletal motion, surface capture</b:BIBTEX_KeyWords></b:Source><b:Source><b:SourceType>Report</b:SourceType><b:Tag>Theobalt2005</b:Tag><b:Title>Joint Motion and Reflectance Capture for Creating Relightable 3D Videos</b:Title><b:Year>2005</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Theobalt</b:Last><b:First>Christian</b:First></b:Person><b:Person><b:Last>Ahmed</b:Last><b:First>Naveed</b:First></b:Person><b:Person><b:Last>Aguiar</b:Last><b:Middle>de</b:Middle><b:First>Edilson</b:First></b:Person><b:Person><b:Last>Ziegler</b:Last><b:First>Gernot</b:First></b:Person><b:Person><b:Last>A.</b:Last><b:Middle>P.</b:Middle><b:First>Hendrik</b:First></b:Person><b:Person><b:Last>Magnor</b:Last><b:First>Marcus</b:First></b:Person><b:Person><b:Last>Seidel</b:Last><b:First>Hans-Peter</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>17</b:Pages><b:Institution>Max-Planck-Institut fuer Informatik</b:Institution><b:ThesisType>Research Report</b:ThesisType><b:BIBTEX_Abstract>3D Videos of Human Actors can be faithfully reconstructed from multiple synchronized video streams by means of a model-based analysis-by-synthesis approach. The reconstructed videos play back in real-time and the virtual viewpoint onto the scene can be arbitrarily changed. By this means authentically animated, photo-realistically and view-dependently textured models of real people can be created that look real under fixed illumination conditions. To import real-world characters into virtual environments, however, also surface reflectance properties must be known. We have thus developed a video-based modeling approach that captures human motion as well as reflectance characteristics from a handful of synchronized video recordings. The presented method [1][2][3] is able to recover spatially varying reflectance properties of clothes by exploiting the time-varying orientation of each surface point with respect to camera and light direction. The resulting model description enables us to match animated subject appearance to different lighting conditions, as well as to interchange surface attributes among different people, e.g. for virtual dressing.</b:BIBTEX_Abstract></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>Levet2007</b:Tag><b:Title>Improved skeleton extraction and surface generation for sketch-based modeling</b:Title><b:Year>2007</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Levet</b:Last><b:First>Florian</b:First></b:Person><b:Person><b:Last>Granier</b:Last><b:First>Xavier</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>27-33</b:Pages><b:Publisher>ACM</b:Publisher><b:BookTitle>GI '07: Proceedings of Graphics Interface 2007</b:BookTitle><b:BIBTEX_Abstract>For the generation of freeform models, sketching interfaces have raised an increasing interest due to their intuitive approach. It is now possible to infer a 3D model directly from a sketched curved. Unfortunately, a limit of current systems is the poor quality of the skeleton automatically extracted from this silhouette, leading to low quality meshes for the resulting objects.

In this paper, we present new solutions that improve the surface generation for sketch-based modeling systems. First, we propose a new algorithm that extracts a smoother skeleton compared to previous approaches. Then, we present a new sampling scheme for the creation of good-quality 3D mesh. Finally, we propose to use a profile curve composed of disconnected components in order to create models which genus is greater than 0.</b:BIBTEX_Abstract></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>Niem1996</b:Tag><b:Title>Camera viewpoint control for the automatic reconstruction of 3D objects</b:Title><b:Year>1996</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Niem</b:Last><b:First>W.</b:First></b:Person><b:Person><b:Last>Steinmetz</b:Last><b:First>M.</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>655-658 vol.3</b:Pages><b:Volume>3</b:Volume><b:BookTitle>Image Processing</b:BookTitle><b:JournalName>Image Processing, 1996. Proceedings., International Conference on</b:JournalName><b:BIBTEX_Abstract>An algorithm for the image dependent control of the camera viewpoint is presented, which is applied to the automatic reconstruction of 3D objects. For computer animation applications, “shape from silhouettes” using equally distributed viewpoints is an often used reconstruction technique. With respect to the local reconstruction errors, the use of equally distributed camera views is unfavourable for arbitrary shaped objects. For that reason, a camera viewpoint control is introduced, which purposefully rotates a turntable with the 3D object depending on the trace of the silhouette contour points over the rotation angle. This trace provides information about the location of object planes and gives a measure for the expected local 3D reconstruction errors. It turns out, that the new algorithm reduces the remaining 3D reconstruction errors up to 70% compared to algorithms without viewpoint control using the same number of viewpoints</b:BIBTEX_Abstract><b:BIBTEX_KeyWords>cameras, computer animation, image reconstruction, image segmentation, motion estimation, telecommunication control3D objects, 3D reconstruction error reduction, algorithm, automatic reconstruction, camera viewpoint control, computer animation applications, image dependent control, local 3D reconstruction errors, local reconstruction errors, object planes location, rotation angle, shape from silhouettes, silhouette contour points, turntable rotation</b:BIBTEX_KeyWords></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>Grauman2003</b:Tag><b:Title>A Bayesian approach to image-based visual hull reconstruction</b:Title><b:Year>2003</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Grauman</b:Last><b:First>K.</b:First></b:Person><b:Person><b:Last>Shakhnarovich</b:Last><b:First>G.</b:First></b:Person><b:Person><b:Last>Darrell</b:Last><b:First>T.</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages> I-187-I-194 vol.1</b:Pages><b:Volume>1</b:Volume><b:BookTitle>Computer Vision and Pattern Recognition</b:BookTitle><b:JournalName>Computer Vision and Pattern Recognition, 2003. Proceedings. 2003 IEEE Computer Society Conference on</b:JournalName><b:BIBTEX_Abstract> We present a Bayesian approach to image-based visual hull reconstruction. The 3D (three-dimensional) shape of an object of a known class is represented by sets of silhouette views simultaneously observed from multiple cameras. We show how the use of a class-specific prior in a visual hull reconstruction can reduce the effect of segmentation errors from the silhouette extraction process. In our representation, 3D information is implicit in the joint observations of multiple contours from known viewpoints. We model the prior density using a probabilistic principal components analysis-based technique and estimate a maximum a posteriori reconstruction of multi-view contours. The proposed method is applied to a dataset of pedestrian images, and improvements in the approximate 3D models under various noise conditions are shown.</b:BIBTEX_Abstract><b:BIBTEX_KeyWords> Bayes methods, computer vision, edge detection, image denoising, image reconstruction, image representation, image segmentation, principal component analysis, stereo image processing 3D model, 3D object shape, Bayesian approach, image reconstruction, image representation, image segmentation, image-based visual hull reconstruction, maximum a posteriori reconstruction, multiple cameras, multiple contours, multiview contour, pedestrian image, probabilistic principal component analysis, silhouette extraction, silhouette view</b:BIBTEX_KeyWords></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>Jin2007</b:Tag><b:Title>Image-based shape model for view-invariant human motion recognition</b:Title><b:Year>2007</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Jin</b:Last><b:First>Ning</b:First></b:Person><b:Person><b:Last>Mokhtarian</b:Last><b:First>F.</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>336-341</b:Pages><b:BookTitle>Advanced Video and Signal Based Surveillance</b:BookTitle><b:JournalName>Advanced Video and Signal Based Surveillance, 2007. AVSS 2007. IEEE Conference on</b:JournalName><b:BIBTEX_Abstract>We propose an image-based shape model for view-invariant human motion recognition. Image-based visual hull explicitly represents the 3D shape of an object, which is computed from a set of silhouettes. We then use the set of silhouettes to implicitly represent the visual hull. Due to the fact that a silhouette is the 2D projection of an object in the 3D world with respect to a certain camera, which is sensitive to the point of view, our multi-silhouette representation for the visual hull entails the correspondence between views. To guarantee the correspondence, we define a canonical multi-camera system and a canonical human body orientation in motions. We then "normalize" all the constructed visual hulls into the canonical multi-camera system, align them to follow the canonical orientation, and finally render them. The rendered views thereby satisfy the requirement of the correspondence. In our visual hull's representation, each silhouette is represented as a fixed number of sampled points on its closed contour, therefore, the 3D shape information is implicitly encoded into the concatenation of multiple 2D contours. Each motion class is then learned by a Hidden Markov Model (HMM) with mixture of Gaussians outputs. Experiments using our algorithm over some data sets give encouraging results.</b:BIBTEX_Abstract><b:BIBTEX_KeyWords>Gaussian processes, cameras, hidden Markov models, image motion analysis, image representation, rendering (computer graphics)3D shape information, Gaussian process, canonical human body orientation, canonical multi camera system, hidden Markov model, image-based shape model, image-based visual hull representation, multi silhouette representation, view-invariant human motion recognition</b:BIBTEX_KeyWords></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>Michel2000</b:Tag><b:Title>Automatic extraction of time-frequency skeletons with minimal spanning trees</b:Title><b:Year>2000</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Michel</b:Last><b:First>O.</b:First></b:Person><b:Person><b:Last>Flandrin</b:Last><b:First>P.</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>89-92</b:Pages><b:Publisher>IEEE Computer Society</b:Publisher><b:BookTitle>ICASSP '00: Proceedings of the Acoustics, Speech, and Signal Processing, 2000. on IEEE International Conference</b:BookTitle><b:BIBTEX_Abstract>Theoretical results have been established in non-parametric entropy estimation, based on asymptotic properties of minimal spanning trees (MST). A new application is proposed for the automatic extraction of time-frequency skeletons in the case of multicomponent chirp-like signals. The proposed method makes use of local maxima of a time-frequency distribution (considered as realizations of a 2D or 3D process), and exploits the efficiency of MSTs for density discrimination and clustering.</b:BIBTEX_Abstract></b:Source><b:Source><b:SourceType>JournalArticle</b:SourceType><b:Tag>Bradley2008</b:Tag><b:Title>Markerless Garment Capture</b:Title><b:Year>2008</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Bradley</b:Last><b:First>Derek</b:First></b:Person><b:Person><b:Last>Popa</b:Last><b:First>Tiberiu</b:First></b:Person><b:Person><b:Last>Sheffer</b:Last><b:First>Alla</b:First></b:Person><b:Person><b:Last>Heidrich</b:Last><b:First>Wolfgang</b:First></b:Person><b:Person><b:Last>Boubekeur</b:Last><b:First>Tamy</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>99</b:Pages><b:Volume>27</b:Volume><b:JournalName>ACM Trans. Graphics (Proc. SIGGRAPH)</b:JournalName><b:BIBTEX_Abstract>A lot of research has recently focused on the problem of capturing the geometry and motion of garments. Such work usually relies on special markers printed on the fabric to establish temporally coherent correspondences between points on the garment’s surface at different times. Unfortunately, this approach is tedious and prevents the capture of off-the-shelf clothing made from interesting fabrics.

In this paper, we describe a marker-free approach to capturing garment motion that avoids these downsides. We establish temporally coherent parameterizations between incomplete geometries that we extract at each timestep with a multiview stereo algorithm. We then fill holes in the geometry using a template. This approach, for the first time, allows us to capture the geometry and motion of unpatterned, off-the-shelf garments made from a range of different fabrics.</b:BIBTEX_Abstract></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>Shin2006</b:Tag><b:Title>Triangular Mesh Generation of Octrees of Non-Convex 3D Objects</b:Title><b:Year>2006</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Shin</b:Last><b:First>Dongjoe</b:First></b:Person><b:Person><b:Last>Tjahjadi</b:Last><b:First>Tardi</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>950-953</b:Pages><b:Volume>3</b:Volume><b:BookTitle>Pattern Recognition</b:BookTitle><b:JournalName>Pattern Recognition, 2006. ICPR 2006. 18th International Conference on</b:JournalName><b:BIBTEX_Abstract>A general surface-generating algorithm, the marching cube, produces triangular meshes from octants where the vertices of octants are clearly classified into either inside or outside the object. However, the algorithm is ambiguous for octrees corresponding to non-convex objects generated using a shape from silhouette technique. This paper presents a methodology which involves Delaunay triangulation to generate surface meshes for such octrees. Since the general 3D Delaunay triangulation creates 3D convex hull which consists of tetrahedron meshes, we propose a method which applies the Delaunay algorithm locally in order to deal with non-convex objects. The proposed method first slices an octree and detects the clusters in each slice. All clusters between adjacent slices are linked based on a 3D probability density cube. The Delaunay algorithm is then applied to locally-linked clusters. Finally the accumulation of triangular meshes forms a final non-convex surface mesh</b:BIBTEX_Abstract><b:BIBTEX_KeyWords>computational geometry, mesh generation, object recognition, octrees, pattern clustering, probabilityDelaunay triangulation, marching cube, nonconvex 3D objects, octrees, probability density cube, silhouette technique, surface-generating algorithm, tetrahedron meshes, triangular mesh generation</b:BIBTEX_KeyWords></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>Brand2004</b:Tag><b:Title>Algebraic solution for the visual hull</b:Title><b:Year>2004</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Brand</b:Last><b:First>M.</b:First></b:Person><b:Person><b:Last>Kang</b:Last><b:First>Kongbin</b:First></b:Person><b:Person><b:Last>Cooper</b:Last><b:First>D.B.</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages> I-30-I-35 Vol.1</b:Pages><b:Volume>1</b:Volume><b:BookTitle>Computer Vision and Pattern Recognition</b:BookTitle><b:JournalName>Computer Vision and Pattern Recognition, 2004. CVPR 2004. Proceedings of the 2004 IEEE Computer Society Conference on</b:JournalName><b:BIBTEX_Abstract> We introduce an algebraic dual-space method for reconstructing the visual hull of a three-dimensional object from occluding contours observed in 2D images. The method exploits the differential structure of the manifold rather than parallax geometry, and therefore requires no correspondences. We begin by observing that the set of 2D contour tangents determines a surface in a dual space where each point represents a tangent plane to the original surface. The primal and dual surfaces have a symmetric algebra: A point on one is orthogonal to its dual point and tangent basis on the other. Thus the primal surface can be reconstructed if the local dual tangent basis can be estimated. Typically this is impossible because the dual surface is noisy and riddled with tangent singularities due to self-crossings. We identify a directionally-indexed local tangent basis that is well-defined and estimable everywhere on the dual surface. The estimation procedure handles singularities in the dual surface and degeneracies arising from measurement noise. The resulting method has O(N) complexity for N observed contour points and gives asymptotically exact reconstructions of surfaces that are totally observable from occluding contours.</b:BIBTEX_Abstract><b:BIBTEX_KeyWords> algebra, computational complexity, computational geometry, differential geometry, image reconstruction 2D contour tangents, O(N) complexity, algebraic dual space method, algebraic solution, directionally indexed local tangent basis, image reconstruction, measurement noise, occluding contours, parallax geometry, symmetric algebra, three dimensional object, visual hull reconstruction</b:BIBTEX_KeyWords></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>Mundermann2007</b:Tag><b:Title>Accurately measuring human movement using articulated ICP with soft-joint constraints and a repository of articulated models</b:Title><b:Year>2007</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Mundermann</b:Last><b:First>L.</b:First></b:Person><b:Person><b:Last>Corazza</b:Last><b:First>S.</b:First></b:Person><b:Person><b:Last>Andriacchi</b:Last><b:First>T.P.</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>1-6</b:Pages><b:BookTitle>Computer Vision and Pattern Recognition</b:BookTitle><b:JournalName>Computer Vision and Pattern Recognition, 2007. CVPR '07. IEEE Conference on</b:JournalName><b:BIBTEX_Abstract>A novel approach for accurate markerless motion capture combining a precise tracking algorithm with a database of articulated models is presented. The tracking approach employs an articulated iterative closest point algorithm with soft-joint constraints for tracking body segments in visual hull sequences. The database of articulated models is derived from a combination of human shapes and anthropometric data, contains a large variety of models and closely mimics variations found in the human population. The database provides articulated models that closely match the outer appearance of the visual hulls, e.g. matches overall height and volume. This information is paired with a kinematic chain enhanced through anthropometric regression equations. Deviations in the kinematic chain from true joint center locations are compensated by the soft-joint constraints approach. As a result accurate and a more anatomical correct outcome is obtained suitable for biomechanical and clinical applications. Joint kinematics obtained using this approach closely matched joint kinematics obtained from a marker based motion capture system.</b:BIBTEX_Abstract><b:BIBTEX_KeyWords>image motion analysis, image segmentation, iterative methods, regression analysisanthropometric regression equations, articulated ICP, articulated model repository, body segment tracking, human movement, iterative closest point algorithm, joint center locations, joint kinematics, motion capture system, soft-joint constraints, soft-joint constraints approach, tracking algorithm</b:BIBTEX_KeyWords></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>Cheung2003a</b:Tag><b:Title>Visual hull alignment and refinement across time: a 3D reconstruction algorithm combining shape-from-silhouette with stereo</b:Title><b:Year>2003</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Cheung</b:Last><b:First>G.K.M.</b:First></b:Person><b:Person><b:Last>Baker</b:Last><b:First>S.</b:First></b:Person><b:Person><b:Last>Kanade</b:Last><b:First>T.</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages> II-375-82 vol.2</b:Pages><b:Volume>2</b:Volume><b:BookTitle>Computer Vision and Pattern Recognition, 2003. Proceedings. 2003 IEEE Computer Society Conference on</b:BookTitle><b:JournalName>Computer Vision and Pattern Recognition, 2003. Proceedings. 2003 IEEE Computer Society Conference on</b:JournalName><b:BIBTEX_Abstract>Visual hull (VH) construction from silhouette images is a popular method of shape estimation. The method, also known as shape-from-silhouette (SFS), is used in many applications such as non-invasive 3D model acquisition, obstacle avoidance, and more recently human motion tracking and analysis. One of the limitations of SFS, however, is that the approximated shape can be very coarse when there are only a few cameras. In this paper, we propose an algorithm to improve the shape approximation by combining multiple silhouette images captured across time. The improvement is achieved by first estimating the rigid motion between the visual hulls formed at different time instants (visual hull alignment) and then combining them (visual hull refinement) to get a tighter bound on the object's shape. Our algorithm first constructs a representation of the VHs called the bounding edge representation. Utilizing a fundamental property of visual hulls, which states that each bounding edge must touch the object at at least one point, we use multi-view stereo to extract points called colored surface points (CSP) on the surface of the object. These CSPs are then used in a 3D image alignment algorithm to find the 6 DOF rigid motion between two visual hulls. Once the rigid motion across time is known, all of the silhouette images are treated as being captured at the same time instant and the shape of the object is refined. We validate our algorithm on both synthetic and real data and compare it with space carving.</b:BIBTEX_Abstract><b:BIBTEX_KeyWords> computer vision, edge detection, image colour analysis, image reconstruction, motion estimation, object detection, stereo image processing 3D image alignment algorithm, 3D reconstruction algorithm, 6 DOF rigid motion, algorithm validation, bounding edge representation, camera, colored surface points, computer vision, human motion tracking, motion analysis, multiview stereo, noninvasive 3D model acquisition, object shape, object surface, obstacle avoidance, real data, rigid motion estimation, shape approximation, shape estimation, shape-from-silhouette, silhouette image, space carving, stereo image, synthetic data, time instant, visual hull alignment, visual hull refinement</b:BIBTEX_KeyWords></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>Katz2003</b:Tag><b:Title>Hierarchical mesh decomposition using fuzzy clustering and cuts</b:Title><b:Year>2003</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Katz</b:Last><b:First>Sagi</b:First></b:Person><b:Person><b:Last>Tal</b:Last><b:First>Ayellet</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>954-961</b:Pages><b:Publisher>ACM</b:Publisher><b:BookTitle>SIGGRAPH '03: ACM SIGGRAPH 2003 Papers</b:BookTitle><b:BIBTEX_Abstract>Cutting up a complex object into simpler sub-objects is a fundamental problem in various disciplines. In image processing, images are segmented while in computational geometry, solid polyhedra are decomposed. In recent years, in computer graphics, polygonal meshes are decomposed into sub-meshes. In this paper we propose a novel hierarchical mesh decomposition algorithm. Our algorithm computes a decomposition into the meaningful components of a given mesh, which generally refers to segmentation at regions of deep concavities. The algorithm also avoids over-segmentation and jaggy boundaries between the components. Finally, we demonstrate the utility of the algorithm in control-skeleton extraction.</b:BIBTEX_Abstract></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>Erol2005</b:Tag><b:Title>Visual Hull Construction Using Adaptive Sampling</b:Title><b:Year>2005</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Erol</b:Last><b:First>A.</b:First></b:Person><b:Person><b:Last>Bebis</b:Last><b:First>G.</b:First></b:Person><b:Person><b:Last>Boyle</b:Last><b:First>R.D.</b:First></b:Person><b:Person><b:Last>Nicolescu</b:Last><b:First>M.</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>234-241</b:Pages><b:Volume>1</b:Volume><b:BookTitle>computer vision</b:BookTitle><b:JournalName>Application of Computer Vision, 2005. WACV/MOTIONS '05 Volume 1. Seventh IEEE Workshops on</b:JournalName><b:BIBTEX_Abstract>Volumetric visual hulls have become very popular in many computer vision applications including human body pose estimation and virtualized reality. In these applications, the visual hull is used to approximate the 3D geometry of an object. Existing volumetric visual hull construction techniques, however, produce a 3-color volume data that merely serves as a bounding volume. In other words it lacks an accurate surface representation. Polygonization can produce satisfactory results only at high resolutions. In this study we extend the binary visual hull to an implicit surface in order to capture the geometry of the visual hull itself. In particular, we introduce an octree-based visual hull specific adaptive sampling algorithm to obtain a volumetric representation that provides accuracy proportional to the level of detail. Moreover, we propose a method to process the resulting octree to extract a crack-free polygonal visual hull surface. Experimental results illustrate the performance of the algorithm.</b:BIBTEX_Abstract><b:BIBTEX_KeyWords>computer vision, image representation, image sampling, image sequences, octrees, pose estimation, virtual realityadaptive sampling, computer vision, crack-free polygonal surface, human body pose estimation, octree-based visual hull, polygonization, surface representation, virtualized reality, visual hull construction</b:BIBTEX_KeyWords></b:Source><b:Source><b:SourceType>Misc</b:SourceType><b:Tag>Remondino2003</b:Tag><b:Title>3D Reconstruction of Human Skeleton from Single Images or Monocular Video Sequences</b:Title><b:Year>2003</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Remondino</b:Last><b:First>Fabio</b:First></b:Person><b:Person><b:Last>Roditakis</b:Last><b:First>Andreas</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>100-107</b:Pages><b:JournalName>Pattern Recognition</b:JournalName><b:PublicationTitle>3D Reconstruction of Human Skeleton from Single Images or Monocular Video Sequences</b:PublicationTitle><b:BIBTEX_Abstract>In this paper, we first review the approaches to recover 3D shape and related movements of a human and then we present an easy and reliable approach to recover a 3D model using just one image or monocular video sequence. A simplification of the perspective camera model is required, due to the absence of stereo view. The human figure is reconstructed in a skeleton form and to improve the visual quality, a pre-defined human model is also fitted to the recovered 3D data.</b:BIBTEX_Abstract></b:Source><b:Source><b:SourceType>JournalArticle</b:SourceType><b:Tag>Moeslund2006</b:Tag><b:Title>A survey of advances in vision-based human motion capture and analysis</b:Title><b:Year>2006</b:Year><b:Comments>Special Issue on Modeling People: Vision-based understanding of a person's shape, appearance, movement and behaviour</b:Comments><b:Author><b:Author><b:NameList><b:Person><b:Last>Moeslund</b:Last><b:Middle>B.</b:Middle><b:First>Thomas</b:First></b:Person><b:Person><b:Last>Hilton</b:Last><b:First>Adrian</b:First></b:Person><b:Person><b:Last>Krüger</b:Last><b:First>Volker</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>90-126</b:Pages><b:Volume>104</b:Volume><b:JournalName>Computer Vision and Image Understanding</b:JournalName><b:BIBTEX_Abstract>This survey reviews advances in human motion capture and analysis from 2000 to 2006, following a previous survey of papers up to 2000 [T.B. Moeslund, E. Granum, A survey of computer vision-based human motion capture, Computer Vision and Image Understanding, 81(3) (2001) 231-268.]. Human motion capture continues to be an increasingly active research area in computer vision with over 350 publications over this period. A number of significant research advances are identified together with novel methodologies for automatic initialization, tracking, pose estimation, and movement recognition. Recent research has addressed reliable tracking and pose estimation in natural scenes. Progress has also been made towards automatic understanding of human actions and behavior. This survey reviews recent trends in video-based human capture and analysis, as well as discussing open problems for future research to achieve automatic visual analysis of human movement.</b:BIBTEX_Abstract><b:BIBTEX_KeyWords>Review</b:BIBTEX_KeyWords></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>Magnor2003</b:Tag><b:Title>Capturing the shape of a dynamic world - fast!</b:Title><b:Year>2003</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Magnor</b:Last><b:First>M.</b:First></b:Person><b:Person><b:Last>Seidel</b:Last><b:First>H.-P.</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>3-9</b:Pages><b:BookTitle>Shape Modeling International</b:BookTitle><b:JournalName>Shape Modeling International, 2003</b:JournalName><b:BIBTEX_Abstract>Acquiring online the evolving shape of a dynamic scene from a handful of video streams may be considered one of the most challenging, but at the same time also most auspicious tasks in contemporary computer graphics and computer vision research. The anticipation of revolutionary new applications such as interactive 3D television broadcasts motivates the ongoing work on free-viewpoint video rendering. The paper aims at giving a state-of-progress report on this lively research endeavor. Different acquisition setups and online reconstruction approaches are exemplified. Yielding interactive frame rates, depth map-based techniques, polyhedral as well as volumetric visual hull reconstruction approaches, and combined methods employing visual hull-guided depth map estimation are presented. The experience gained with these approaches allows us to identify future research directions towards real-time analysis and high-quality synthesis of dynamic, real-world scenes.</b:BIBTEX_Abstract><b:BIBTEX_KeyWords> computational geometry, image reconstruction, rendering (computer graphics), video signal processing acquisition setup, computer graphics, computer vision, depth map estimation, depth map-based technique, dynamic scene, dynamic world, evolving shape, free-viewpoint video rendering, high-quality synthesis, interactive 3D television broadcast, interactive frame rate, online reconstruction, polyhedral visual hull reconstruction, real-time analysis, shape capture, state-of-progress report, video stream, volumetric visual hull reconstruction</b:BIBTEX_KeyWords></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>Deng2009</b:Tag><b:Title>Perceptually Consistent Example-based Human Motion Retrieval</b:Title><b:Year>2009</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Deng</b:Last><b:First>Z.</b:First></b:Person><b:Person><b:Last>Gu</b:Last><b:First>Q.</b:First></b:Person><b:Person><b:Last>Li</b:Last><b:First>Q.</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>191-198</b:Pages><b:BookTitle>Proc. of ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games</b:BookTitle><b:BIBTEX_Abstract>Large amount of human motion capture data have been increasingly recorded and used in animation and gaming applications. Efficient retrieval of logically similar motions from a large data repository thereby serves as a fundamental basis for these motion data based applications. In this paper we present a perceptually consistent, example-based human motion retrieval approach that is capable of efficiently searching for and ranking similar motion sequences given a query motion input. Our method employs a motion pattern discovery and matching scheme that breaks human motions into a part-based, hierarchical motion representation. Building upon this representation, a fast string match algorithm is used for efficient runtime motion query processing. Finally, we conducted comparative user studies to evaluate the accuracy and perceptual-consistency of our approach by comparing it with the state of the art example-based human motion search algorithms.</b:BIBTEX_Abstract></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>Yan2008</b:Tag><b:Title>Learning 4D action feature models for arbitrary view action recognition</b:Title><b:Year>2008</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Yan</b:Last><b:First>Pingkun</b:First></b:Person><b:Person><b:Last>Khan</b:Last><b:First>S.M.</b:First></b:Person><b:Person><b:Last>Shah</b:Last><b:First>M.</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>1-7</b:Pages><b:BookTitle>Computer Vision and Pattern Recognition</b:BookTitle><b:JournalName>Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on</b:JournalName><b:BIBTEX_Abstract>In this paper we present a novel approach using a 4D (x,y,z,t) action feature model (4D-AFM) for recognizing actions from arbitrary views. The 4D-AFM elegantly encodes shape and motion of actors observed from multiple views. The modeling process starts with reconstructing 3D visual hulls of actors at each time instant. Spatiotemporal action features are then computed in each view by analyzing the differential geometric properties of spatio-temporal volumes (3D STVs) generated by concatenating the actorpsilas silhouette over the course of the action (x, y, t). These features are mapped to the sequence of 3D visual hulls over time (4D) to build the initial 4D-AFM. Actions are recognized based on the scores of matching action features from the input videos to the model points of 4D-AFMs by exploiting pairwise interactions of features. Promising recognition results have been demonstrated on the multi-view IXMAS dataset using both single and multi-view input videos.</b:BIBTEX_Abstract><b:BIBTEX_KeyWords>image coding, image matching, image recognition, image sequences, video signal processing3D visual hulls, 4D action feature models, 4D-AFM, arbitrary view action recognition, multiview input videos, spatiotemporal action features, spatiotemporal volumes</b:BIBTEX_KeyWords></b:Source><b:Source><b:SourceType>JournalArticle</b:SourceType><b:Tag>Yamazaki2008</b:Tag><b:Title>The Theory and Practice of Coplanar Shadowgram Imaging forÂ Acquiring Visual Hulls of Intricate Objects</b:Title><b:Year>2008</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Yamazaki</b:Last><b:First>Shuntaro</b:First></b:Person><b:Person><b:Last>Narasimhan</b:Last><b:First>Srinivasa</b:First></b:Person><b:Person><b:Last>Baker</b:Last><b:First>Simon</b:First></b:Person><b:Person><b:Last>Kanade</b:Last><b:First>Takeo</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>--</b:Pages><b:Volume>1</b:Volume><b:JournalName>International Journal of Computer Vision</b:JournalName><b:BIBTEX_Abstract>Abstract&amp;nbsp;&amp;nbsp;Acquiring 3D models of intricate objects (like tree branches, bicycles and insects) is a challenging task due to severe self-occlusions, repeated thin structures, and surface discontinuities. In theory, a shape-from-silhouettes (SFS) approach can overcome these difficulties and reconstruct visual hulls that are close to the actual shapes, regardless of the complexity of the object. In practice, however, SFS is highly sensitive to errors in silhouette contours and the calibration of the imaging system, and has therefore not been used for obtaining accurate shapes with a large number of views. In this work, we present a practical approach to SFS using a novel technique called coplanar shadowgram imaging that allows us to use dozens to even hundreds of views for visual hull reconstruction. A point light source is moved around an object and the shadows (silhouettes) cast onto a single background plane are imaged. We characterize this imaging system in terms of image projection, reconstruction ambiguity, epipolar geometry, and shape and source recovery. The coplanarity of the shadowgrams yields unique geometric properties that are not possible in traditional multi-view camera-based imaging systems. These properties allow us to derive a robust and automatic algorithm to recover the visual hull of an object and the 3D positions of the light source simultaneously, regardless of the complexity of the object. We demonstrate the acquisition of several intricate shapes with severe occlusions and thin structures, using 50 to 120 views.</b:BIBTEX_Abstract></b:Source><b:Source><b:SourceType>JournalArticle</b:SourceType><b:Tag>Botsch2008</b:Tag><b:Title>On Linear Variational Surface Deformation Methods</b:Title><b:Year>2008</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Botsch</b:Last><b:First>Mario</b:First></b:Person><b:Person><b:Last>Sorkine</b:Last><b:First>Olga</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>213-230</b:Pages><b:Volume>14</b:Volume><b:Publisher>IEEE Educational Activities Department</b:Publisher><b:JournalName>IEEE Transactions on Visualization and Computer Graphics</b:JournalName><b:BIBTEX_Abstract>This survey reviews the recent advances in linear variational mesh deformation techniques. These methods were developed for editing detailed high-resolution meshes, like those produced by scanning real-world objects. The challenge of manipulating such complex surfaces is three-fold: the deformation technique has to be sufficiently fast, robust, and intuitive and easy to control to be useful for interactive applications. An intuitive, and thus predictable, deformation tool should provide physically plausible and aesthetically pleasing surface deformations, which in particular requires its geometric details to be preserved. The methods we survey generally formulate surface deformation as a global variational optimization problem that addresses the differential properties of the edited surface. Efficiency and robustness are achieved by linearizing the underlying objective functional, such that the global optimization amounts to solving a sparse linear system of equations. We review the different deformation energies and detail preservation techniques that were proposed in the recent years, together with the various techniques to rectify the linearization artifacts. Our goal is to provide the reader with a systematic classification and comparative description of the different techniques, revealing the strengths and weaknesses of each approach in common editing scenarios.</b:BIBTEX_Abstract></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>Ishikawa2005</b:Tag><b:Title>Real-time generation of novel views of a dynamic scene using morphing and visual hull</b:Title><b:Year>2005</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Ishikawa</b:Last><b:First>T.</b:First></b:Person><b:Person><b:Last>Yamazawa</b:Last><b:First>K.</b:First></b:Person><b:Person><b:Last>Yokoya</b:Last><b:First>N.</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages> I-1013-16</b:Pages><b:Volume>1</b:Volume><b:BookTitle>Image Processing</b:BookTitle><b:JournalName>Image Processing, 2005. ICIP 2005. IEEE International Conference on</b:JournalName><b:BIBTEX_Abstract>Recently, generation of novel views from images acquired by multiple cameras has been investigated. It can be applied to telepresence effectively. Most conventional methods need some assumptions about the scene such as a static scene and limited positions of objects. In this paper, we propose a new method for generating novel view images of a dynamic scene with a wide view, which does not depend on the scene. The images acquired from omni-directional cameras are first divided into static regions and dynamic regions. The novel view images are then generated by applying a morphing technique to static regions and by computing visual hulls for dynamic regions in real-time. In experiments, we show that a prototype system can generate novel view images in real-time from live video streams.</b:BIBTEX_Abstract><b:BIBTEX_KeyWords> cameras, image processing, video streaming dynamic scene, morphing technique, omnidirectional cameras, video streams, view images, visual hull</b:BIBTEX_KeyWords></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>Aguiar2007</b:Tag><b:Title>Marker-less Deformable Mesh Tracking for Human Shape and Motion Capture</b:Title><b:Year>2007</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Aguiar</b:Last><b:Middle>de</b:Middle><b:First>E.</b:First></b:Person><b:Person><b:Last>Theobalt</b:Last><b:First>C.</b:First></b:Person><b:Person><b:Last>Stoll</b:Last><b:First>C.</b:First></b:Person><b:Person><b:Last>Seidel</b:Last><b:Middle>P.</b:Middle><b:First>H.</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>1-8</b:Pages><b:BookTitle>Computer Vision and Pattern Recognition, 2007. CVPR '07. IEEE Conference on</b:BookTitle><b:JournalName>Computer Vision and Pattern Recognition, 2007. CVPR '07. IEEE Conference on</b:JournalName><b:BIBTEX_Abstract>We present a novel algorithm to jointly capture the motion and the dynamic shape of humans from multiple video streams without using optical markers. Instead of relying on kinematic skeletons, as traditional motion capture methods, our approach uses a deformable high-quality mesh of a human as scene representation. It jointly uses an image-based 3D correspondence estimation algorithm and a fast Laplacian mesh deformation scheme to capture both motion and surface deformation of the actor from the input video footage. As opposed to many related methods, our algorithm can track people wearing wide apparel, it can straightforwardly be applied to any type of subject, e.g. animals, and it preserves the connectivity of the mesh over time. We demonstrate the performance of our approach using synthetic and captured real-world video sequences and validate its accuracy by comparison to the ground truth.</b:BIBTEX_Abstract><b:BIBTEX_KeyWords>capture, cg</b:BIBTEX_KeyWords></b:Source><b:Source><b:SourceType>JournalArticle</b:SourceType><b:Tag>Shapira2008</b:Tag><b:Title>Consistent mesh partitioning and skeletonisation using the shape diameter function</b:Title><b:Year>2008</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Shapira</b:Last><b:First>Lior</b:First></b:Person><b:Person><b:Last>Shamir</b:Last><b:First>Ariel</b:First></b:Person><b:Person><b:Last>Cohen-Or</b:Last><b:First>Daniel</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>249-259</b:Pages><b:Volume>24</b:Volume><b:Publisher>Springer-Verlag New York, Inc.</b:Publisher><b:JournalName>Vis. Comput.</b:JournalName><b:BIBTEX_Abstract>Mesh partitioning and skeletonisation are fundamental for many computer graphics and animation techniques. Because of the close link between an object’s skeleton and its boundary, these two problems are in many cases complementary. Any partitioning of the object can assist in the creation of a skeleton and any segmentation of the skeleton can infer a partitioning of the object. In this paper, we consider these two problems on a wide variety of meshes, and strive to construct partitioning and skeletons which remain consistent across a family of objects, not a single one. Such families can consist of either a single object in multiple poses and resolutions, or multiple objects which have a general common shape. To achieve consistency, we base our algorithms on a volume-based shape-function called the shape-diameter-function (SDF), which remains largely oblivious to pose changes of the same object and maintains similar values in analogue parts of different objects. The SDF is a scalar function defined on the mesh surface; however, it expresses a measure of the diameter of the object’s volume in the neighborhood of each point on the surface. Using the SDF we are able to process and manipulate families of objects which contain similarities using a simple and consistent algorithm: consistently partitioning and creating skeletons among multiple meshes.</b:BIBTEX_Abstract></b:Source><b:Source><b:SourceType>JournalArticle</b:SourceType><b:Tag>Paris2004</b:Tag><b:Title>Capture of hair geometry from multiple images</b:Title><b:Year>2004</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Paris</b:Last><b:First>Sylvain</b:First></b:Person><b:Person><b:Last>Brice\,</b:Last><b:Middle>M.</b:Middle><b:First>Hector</b:First></b:Person><b:Person><b:Last>Sillion</b:Last><b:Middle>X.</b:Middle><b:First>Fran\c{c}ois</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>712-719</b:Pages><b:Volume>23</b:Volume><b:Publisher>ACM</b:Publisher><b:JournalName>ACM Trans. Graph.</b:JournalName><b:BIBTEX_Abstract>Hair is a major feature of digital characters. Unfortunately, it has a complex geometry which challenges standard modeling tools. Some dedicated techniques exist, but creating a realistic hairstyle still takes hours. Complementary to user-driven methods, we here propose an image-based approach to capture the geometry of hair.The novelty of this work is that we draw information from the scattering properties of the hair that are normally considered a hindrance. To do so, we analyze image sequences from a fixed camera with a moving light source. We first introduce a novel method to compute the image orientation of the hairs from their anisotropic behavior. This method is proven to subsume and extend existing work while improving accuracy. This image orientation is then raised into a 3D orientation by analyzing the light reflected by the hair fibers. This part relies on minimal assumptions that have been proven correct in previous work.Finally, we show how to use several such image sequences to reconstruct the complete hair geometry of a real person. Results are shown to illustrate the fidelity of the captured geometry to the original hair. This technique paves the way for a new approach to digital hair generation.</b:BIBTEX_Abstract></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>Miller2007</b:Tag><b:Title>Safe hulls</b:Title><b:Year>2007</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Miller</b:Last><b:First>G.</b:First></b:Person><b:Person><b:Last>Hilton</b:Last><b:First>A.</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>1-8</b:Pages><b:BookTitle>Visual Media Production</b:BookTitle><b:JournalName>Visual Media Production, 2007. IETCVMP. 4th European Conference on</b:JournalName><b:BIBTEX_Abstract>The visual hull is widely used as a proxy for novel view synthesis in computer vision. This paper introduces the safe hull, the first visual hull reconstruction technique to produce a surface containing only foreground parts. A theoretical basis underlies this novel approach which, unlike any previous work, can also identify phantom volumes attached to real objects. Using an image-based method, the visual hull is constructed with respect to each real view and used to identify safe zones in the original silhouettes. The safe zones define volumes known to only contain surface corresponding to a real object. The zones are used in a second reconstruction step to produce a surface without phantom volumes. Results demonstrate the effectiveness of this method for improving surface shape and scene realism, and its advantages over heuristic techniques.</b:BIBTEX_Abstract><b:BIBTEX_KeyWords>computer vision, image reconstructioncomputer vision, image-based method, phantom volumes identification, safe hull, view synthesis, visual hull reconstruction</b:BIBTEX_KeyWords></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>Li2002</b:Tag><b:Title>Combining stereo and visual hull information for on-line reconstruction and rendering of dynamic scenes</b:Title><b:Year>2002</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Li</b:Last><b:First>Ming</b:First></b:Person><b:Person><b:Last>Schirmacher</b:Last><b:First>H.</b:First></b:Person><b:Person><b:Last>Magnor</b:Last><b:First>M.</b:First></b:Person><b:Person><b:Last>Siedel</b:Last><b:First>H.-P.</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>9-12</b:Pages><b:BookTitle>Multimedia Signal Processing</b:BookTitle><b:JournalName>Multimedia Signal Processing, 2002 IEEE Workshop on</b:JournalName><b:BIBTEX_Abstract>In this paper, we present a novel system which, combines depth-from-stereo and visual hull reconstruction for acquiring dynamic real-world scenes at interactive rates. First, we use the silhouettes from multiple views to construct a polyhedral visual hull is then used to limit the disparity range during depth-from-stereo computation. The restricted search range improves both speed and quality of the stereo reconstruction. In return, stereo information can compensate for some of the visual hull method, such as inability to reconstruct surface details and concave regions. Our system achieves a reconstruction frame rate of 4fps.</b:BIBTEX_Abstract><b:BIBTEX_KeyWords> image reconstruction, rendering (computer graphics), stereo image processing depth-from-stereo computation, disparity range, dynamic scene rendering, frame rate reconstruction, interactive rates, on-line reconstruction, polyhedral visual hull, stereo reconstruction, visual hull information, visual hull reconstruction</b:BIBTEX_KeyWords></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>Oda2006</b:Tag><b:Title>Interactive skeleton extraction for 3D animation using geodesic distances</b:Title><b:Year>2006</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Oda</b:Last><b:First>Takuya</b:First></b:Person><b:Person><b:Last>Itoh</b:Last><b:First>Yuichi</b:First></b:Person><b:Person><b:Last>Nakai</b:Last><b:First>Wataru</b:First></b:Person><b:Person><b:Last>Nomura</b:Last><b:First>Katsuhiro</b:First></b:Person><b:Person><b:Last>Kitamura</b:Last><b:First>Yoshifumi</b:First></b:Person><b:Person><b:Last>Kishino</b:Last><b:First>Fumio</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>9</b:Pages><b:Publisher>ACM</b:Publisher><b:BookTitle>SIGGRAPH '06: ACM SIGGRAPH 2006 Research posters</b:BookTitle><b:BIBTEX_Abstract>This paper proposes a method of extracting skeleton interactively for 3D character animation. A skeleton is automatically and interactively generated from the object data of 3D models in a process that consists of ?ve steps: 1) transformation into a low polygon model from the original model composed of a large number of polygons; 2) calculation of the sum of the geodesic distance from speci?c points to all vertices on the 3D model; 3) subdivision of the 3D model using the sum of geodesic distances; 4) generation of skeleton joints (skeleton nodes) on each boundary surface between those subdivision areas; and 5) connection of all skeleton nodes. We also propose a method for ?exible skeleton generation using boundary shapes and the sum of geodesic distances. After generating the skeleton, various animations can be created by interpolating key poses created from user manipulation of skeleton joints. Since skeletons can be generated with this method where speci?ed by users, users are expected to create interactively ?exible animations of 3D models</b:BIBTEX_Abstract></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>Gong2009</b:Tag><b:Title>3D Mesh Skeleton Extraction Based on Feature Points</b:Title><b:Year>2009</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Gong</b:Last><b:First>Faming</b:First></b:Person><b:Person><b:Last>Kang</b:Last><b:First>Cui</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>326-329</b:Pages><b:Volume>1</b:Volume><b:BookTitle>Computer Engineering and Technology</b:BookTitle><b:JournalName>Computer Engineering and Technology, 2009. ICCET '08. International Conference on</b:JournalName><b:BIBTEX_Abstract>A novel efficient skeleton extraction algorithm is proposed, which is based on feature points extraction and Reeb graph theories. Because of the topological facility of feature points, a model can be divided into several branches according to them. One feature point can present one branch. So we just extract the other point of the branch - that could be skeleton point, connect these points with their corresponding feature points, then we can get all branch skeletons. Finally, connecting branch skeletons through connecting skeleton points according the topological relationship of each skeleton point preserving, then 3D modelpsilas skeleton can be extracted.Without pre-processing stages and without input parameters, this algorithm can automatically extract the skeleton of 3D models. Theoretical analyses and experimental results show that our method has a lower computing complexity, and meets the requirement of extracting nice-looking and affine-invariant skeletons efficiently.</b:BIBTEX_Abstract><b:BIBTEX_KeyWords>computational complexity, feature extraction3D mesh skeleton extraction, Reeb graph theories, affine-invariant skeletons, computing complexity, feature points extraction</b:BIBTEX_KeyWords></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>Zhou2008</b:Tag><b:Title>Reconstruction of the Visual Hull with Modified Ray-tracing and Fast Slice-based Surface Extraction</b:Title><b:Year>2008</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Zhou</b:Last><b:First>Jie</b:First></b:Person><b:Person><b:Last>Chen</b:Last><b:First>Hai</b:First></b:Person><b:Person><b:Last>Chen</b:Last><b:First>Yue</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>907-912</b:Pages><b:BookTitle>Young Computer Scientists</b:BookTitle><b:JournalName>Young Computer Scientists, 2008. ICYCS 2008. The 9th International Conference for</b:JournalName><b:BIBTEX_Abstract>This paper presents a novel method for constructing a surface mesh from silhouettes estimated from image sequences. We implement a modified ray-tracing algorithm with an epipolar-line examination to obtain the surface vertices and investigate surface topology information by a slice-based surface extraction algorithm. In the ray tracing stage, a 2D-vector-based sandwich test is designed to predict the results of intersection, while culling only needs sequence calculations. Both theoretical analysis and experiment results show that our method is significantly faster than the original ray-tracing algorithm without losing accuracy.</b:BIBTEX_Abstract><b:BIBTEX_KeyWords>computational geometry, feature extraction, image reconstruction, image sequences, mesh generation, ray tracing, surface fitting2D vector-based sandwich test, epipolar-line examination, image sequence, ray tracing, slice-based surface extraction, surface mesh construction, surface topology information, visual hull reconstruction</b:BIBTEX_KeyWords></b:Source><b:Source><b:SourceType>Misc</b:SourceType><b:Tag>Forbes2006</b:Tag><b:Title>Shape-from-Silhouette with Two Mirrors and an Uncalibrated Camera</b:Title><b:Year>2006</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Forbes</b:Last><b:First>Keith</b:First></b:Person><b:Person><b:Last>Nicolls</b:Last><b:First>Fred</b:First></b:Person><b:Person><b:Last>Jager</b:Last><b:Middle>de</b:Middle><b:First>Gerhard</b:First></b:Person><b:Person><b:Last>Voigt</b:Last><b:First>Anthon</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>165-178</b:Pages><b:JournalName>Computer Vision â€“ ECCV 2006</b:JournalName><b:PublicationTitle>Shape-from-Silhouette with Two Mirrors and an Uncalibrated Camera</b:PublicationTitle><b:BIBTEX_Abstract>Two planar mirrors are positioned to show five views of an object, and snapshots are captured from different viewpoints. We present closed form solutions for calculating the focal length, principal point, mirror and camera poses directly from the silhouette outlines of the object and its reflections. In the noisy case, these equations are used to form initial parameter estimates that are refined using iterative minimisation. The self-calibration allows the visual cones from each silhouette to be specified in a common reference frame so that the visual hull can be constructed. The proposed setup provides a simple method for creating 3D multimedia content that does not rely on specialised equipment. Experimental results demonstrate the reconstruction of a toy horse and a locust from real images. Synthetic images are used to quantify the sensitivity of the self-calibration to quantisation noise. In terms of the silhouette calibration ratio, degradation in silhouette quality has a greater effect on silhouette set consistency than computed calibration parameters.</b:BIBTEX_Abstract></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>Miller2006</b:Tag><b:Title>Exact View-Dependent Visual Hulls</b:Title><b:Year>2006</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Miller</b:Last><b:First>G.</b:First></b:Person><b:Person><b:Last>Hilton</b:Last><b:First>A.</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>107-111</b:Pages><b:Volume>1</b:Volume><b:BookTitle>Pattern Recognition</b:BookTitle><b:JournalName>Pattern Recognition, 2006. ICPR 2006. 18th International Conference on</b:JournalName><b:BIBTEX_Abstract>The visual hull is widely used to produce three dimensional models from multiple views, due to the reliability of the resulting surface. This paper presents a novel method for efficiently evaluating the exact view-dependent visual hull without using approximations. Methods for selecting intersections and ordering them via the cross ratio are presented. Results show the high quality of the surfaces produced using this method</b:BIBTEX_Abstract><b:BIBTEX_KeyWords>image reconstruction, stereo image processing3D models, shape based reconstruction, surface reconstruction, view-dependent visual hulls</b:BIBTEX_KeyWords></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>Franco2006</b:Tag><b:Title>Visual Shapes of Silhouette Sets</b:Title><b:Year>2006</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Franco</b:Last><b:First>J.-S.</b:First></b:Person><b:Person><b:Last>Lapierre</b:Last><b:First>M.</b:First></b:Person><b:Person><b:Last>Boyer</b:Last><b:First>E.</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>397-404</b:Pages><b:BookTitle>3D Data Processing</b:BookTitle><b:JournalName>3D Data Processing, Visualization, and Transmission, Third International Symposium on</b:JournalName><b:BIBTEX_Abstract>Shape from silhouette methods are extensively used to model dynamic and non-rigid objects using binary foreground-background images. Since the problem of reconstructing shapes from silhouettes is ambiguous, a number of solutions exist and several approaches only consider the one with a maximal volume, called the visual hull. However, the visual hull is not always a good approximation of shapes, in particular when observing smooth surfaces with few cameras. In this paper, we consider instead a class of solutions to the silhouette reconstruction problem that we call visual shapes. Such a class includes the visual hull, but also better approximations of the observed shapes which can take into account local assumptions such as smoothness, among others. Our contributions with respect to existing works is first to identify silhouette consistent shapes different from the visual hull, and second to give a practical way to estimate such shapes in real time. Experiments on various sets of data including human body silhouettes are shown to illustrate the principle and the interests of visual shapes.</b:BIBTEX_Abstract><b:BIBTEX_KeyWords>image reconstruction3D model, binary foreground-background image, image representation, image texture, shape reconstruction, silhouette method, silhouette reconstruction problem, visual hull, visual shapes</b:BIBTEX_KeyWords></b:Source><b:Source><b:SourceType>JournalArticle</b:SourceType><b:Tag>Campos2006a</b:Tag><b:Title>Regression-based Hand Pose Estimation from Multiple Cameras</b:Title><b:Year>2006</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>de</b:Last><b:Middle>E.</b:Middle><b:First>Teofilo</b:First></b:Person><b:Person><b:Last>Murray</b:Last><b:Middle>W.</b:Middle><b:First>David</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>782-789</b:Pages><b:Volume>1</b:Volume><b:Publisher>IEEE Computer Society</b:Publisher><b:JournalName>Computer Vision and Pattern Recognition, IEEE Computer Society Conference on</b:JournalName><b:BIBTEX_Abstract>The RVM-based learning method for whole body pose estimation proposed by Agarwal and Triggs is adapted to hand pose recovery. To help overcome the difficulties presented by the greater degree of self-occlusion and the wider range of poses exhibited in hand imagery, the adaptation proposes a method for combining multiple views. Comparisons of performance using single versus multiple views are reported for both synthesized and real imagery, and the effects of the number of image measurements and the number of training samples on performance are explored.</b:BIBTEX_Abstract></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>Flagg2009</b:Tag><b:Title>Human video textures</b:Title><b:Year>2009</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Flagg</b:Last><b:First>Matthew</b:First></b:Person><b:Person><b:Last>Nakazawa</b:Last><b:First>Atsushi</b:First></b:Person><b:Person><b:Last>Zhang</b:Last><b:First>Qiushuang</b:First></b:Person><b:Person><b:Last>Kang</b:Last><b:Middle>Bing</b:Middle><b:First>Sing</b:First></b:Person><b:Person><b:Last>Ryu</b:Last><b:Middle>Kee</b:Middle><b:First>Young</b:First></b:Person><b:Person><b:Last>Essa</b:Last><b:First>Irfan</b:First></b:Person><b:Person><b:Last>Rehg</b:Last><b:Middle>M.</b:Middle><b:First>James</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>199-206</b:Pages><b:Publisher>ACM</b:Publisher><b:BookTitle>I3D '09: Proceedings of the 2009 symposium on Interactive 3D graphics and games</b:BookTitle><b:BIBTEX_Abstract>This paper describes a data-driven approach for generating photorealistic animations of human motion. Each animation sequence follows a user-choreographed path and plays continuously by seamlessly transitioning between different segments of the captured data. To produce these animations, we capitalize on the complementary characteristics of motion capture data and video. We customize our capture system to record motion capture data that are synchronized with our video source. Candidate transition points in video clips are identified using a new similarity metric based on 3-D marker trajectories and their 2-D projections into video. Once the transitions have been identified, a video-based motion graph is constructed. We further exploit hybrid motion and video data to ensure that the transitions are seamless when generating animations. Motion capture marker projections serve as control points for segmentation of layers and nonrigid transformation of regions. This allows warping and blending to generate seamless in-between frames for animation. We show a series of choreographed animations of walks and martial arts scenes as validation of our approach.</b:BIBTEX_Abstract></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>Sumner2007</b:Tag><b:Title>Embedded deformation for shape manipulation</b:Title><b:Year>2007</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Sumner</b:Last><b:Middle>W.</b:Middle><b:First>Robert</b:First></b:Person><b:Person><b:Last>Schmid</b:Last><b:First>Johannes</b:First></b:Person><b:Person><b:Last>Pauly</b:Last><b:First>Mark</b:First></b:Person></b:NameList></b:Author></b:Author><b:Publisher>ACM</b:Publisher><b:BookTitle>SIGGRAPH '07: ACM SIGGRAPH 2007 papers</b:BookTitle><b:BIBTEX_Abstract>We present an algorithm that generates natural and intuitive deformations via direct manipulation for a wide range of shape representations and editing scenarios. Our method builds a space deformation represented by a collection of affine transformations organized in a graph structure. One transformation is associated with each graph node and applies a deformation to the nearby space. Positional constraints are specified on the points of an embedded object. As the user manipulates the constraints, a nonlinear minimization problem is solved to find optimal values for the affine transformations. Feature preservation is encoded directly in the objective function by measuring the deviation of each transformation from a true rotation. This algorithm addresses the problem of "embedded deformation" since it deforms space through direct manipulation of objects embedded within it, while preserving the embedded objects' features. We demonstrate our method by editing meshes, polygon soups, mesh animations, and animated particle systems.</b:BIBTEX_Abstract><b:BIBTEX_KeyWords>registration</b:BIBTEX_KeyWords></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>Boehnen2009</b:Tag><b:Title>3D Signatures for Fast 3D Face Recognition</b:Title><b:Year>2009</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Boehnen</b:Last><b:First>Chris</b:First></b:Person><b:Person><b:Last>Peters</b:Last><b:First>Tanya</b:First></b:Person><b:Person><b:Last>Flynn</b:Last><b:First>Patrick</b:First></b:Person></b:NameList></b:Author></b:Author><b:BookTitle>Proceedings of International Conference on Biometrics (ICB) 2009</b:BookTitle><b:BIBTEX_Abstract>We propose a vector representation (called a 3D signature) for 3D face shape in biometrics applications. Elements of the vector correspond to fixed surface points in a face-centered coordinate system. Since the elements are registered to the face comparisons of vectors to produce match scores can be performed without a probe to gallery alignment step such as an invocation of the iterated closest point (ICPalgorithm in the calculation of each match score. The proposed 3D face recognition method employing the 3D signature ran more than three orders of magnitude faster than a traditional ICP based distance implementation, without sacrificing accuracy. As a result, it is feasible to apply distance based 3D face biometrics to recognition scenarios that, because of computational constraints, may have previously been limited to verification. Our use of more complex shape regions, which is a trivial task with the use of 3D signatures, improves biometric performance over simple spherical cut regions used previously [1]. Experimental results with a large database of 3D images demonstrate the technique and its advantages.</b:BIBTEX_Abstract></b:Source><b:Source><b:SourceType>JournalArticle</b:SourceType><b:Tag>Holroyd2008</b:Tag><b:Title>A Photometric Approach for Estimating Normals and Tangents</b:Title><b:Year>2008</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Holroyd</b:Last><b:First>Michael</b:First></b:Person><b:Person><b:Last>Lawrence</b:Last><b:First>Jason</b:First></b:Person><b:Person><b:Last>Humphreys</b:Last><b:First>Greg</b:First></b:Person><b:Person><b:Last>Zickler</b:Last><b:First>Todd</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>133</b:Pages><b:Volume>27</b:Volume><b:JournalName>ACM Transactions on Graphics (Proceedings of SIGGRAPH Asia 2008)</b:JournalName><b:BIBTEX_Abstract>This paper presents a novel technique for acquiring the shape of real-world objects with complex isotropic and anisotropic reflectance. Our method estimates the local normal and tangent vectors at each pixel in a reference view from a sequence of images taken under varying point lighting. We show that for many real-world materials and a restricted set of light positions, the 2D slice of the BRDF obtained by fixing the local view direction is symmetric under reflections of the halfway vector across the normal-tangent and normal-binormal planes. Based on this analysis, we develop an optimization that estimates the local surface frame by identifying these planes of symmetry in the measured BRDF. As with other photometric methods, a key benefit of our approach is that the input is easy to acquire and is less sensitive to calibration errors than stereo or multi-view techniques. Unlike prior work, our approach allows estimating the surface tangent in the case of anisotropic reflectance. We confirm the accuracy and reliability of our approach with analytic and measured data, present several normal and tangent fields acquired with our technique, and demonstrate applications to image relighting and appearance editing.</b:BIBTEX_Abstract></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>Liu2006</b:Tag><b:Title>A Novel Volumetric Shape from Silhouette Algorithm Based on a Centripetal Pentahedron Model</b:Title><b:Year>2006</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Liu</b:Last><b:First>Xin</b:First></b:Person><b:Person><b:Last>Yao</b:Last><b:First>Hongxun</b:First></b:Person><b:Person><b:Last>Yao</b:Last><b:First>Guilin</b:First></b:Person><b:Person><b:Last>Gao</b:Last><b:First>Wen</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>9-9</b:Pages><b:Volume>1</b:Volume><b:BookTitle>Pattern Recognition</b:BookTitle><b:JournalName>Pattern Recognition, 2006. ICPR 2006. 18th International Conference on</b:JournalName><b:BIBTEX_Abstract>In this paper we present a novel volumetric shape from silhouette algorithm based on a centripetal pentahedron model. The algorithm first partitions the space with a set of infinite triangular pyramids derived from a geodesic sphere. Then the pyramids are cut by silhouettes into a set of pentahedrons, which together constitute the centripetal pentahedron model of the visual hull. This process is accelerated by pre-computed polar silhouette graphs (PSGs) and reduced PSGs. Finally a mesh surface model is extracted by marching pentahedrons. Our algorithm has the advantages of robustness, speediness and preciseness</b:BIBTEX_Abstract><b:BIBTEX_KeyWords>computational geometry, feature extraction, graph theory, image resolution, stereo image processingcentripetal pentahedron model, geodesic sphere, infinite triangular pyramids, marching pentahedrons, mesh surface model extraction, polar silhouette graphs, silhouette algorithm, space partitioning, visual hull, volumetric shape</b:BIBTEX_KeyWords></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>Yue2003</b:Tag><b:Title>View synthesis of articulating humans using visual hull</b:Title><b:Year>2003</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Yue</b:Last><b:First>Zhanfeng</b:First></b:Person><b:Person><b:Last>Zhao</b:Last><b:First>Liang</b:First></b:Person><b:Person><b:Last>Chellappa</b:Last><b:First>R.</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages> I-489-92 vol.1</b:Pages><b:Volume>1</b:Volume><b:BookTitle>Multimedia and Expo</b:BookTitle><b:JournalName>Multimedia and Expo, 2003. ICME '03. Proceedings. 2003 International Conference on</b:JournalName><b:BIBTEX_Abstract> In this paper, we present a method, which combines image-based visual hull and human body part segmentation for overcoming the inability of the visual hull method to reconstruct concave regions. The virtual silhouette image corresponding to the given viewing direction is first produced with image-based visual hull. Human body part localization technique is used to segment the input images and the rendered virtual silhouette image into convex body parts. The body parts in the virtual view are generated separately from the corresponding body parts in the input views and then assembled together. The previously rendered silhouette image is used to locate the corresponding body parts in input views and avoid the unconnected or squeezed regions in the assembled final view. Experiments show that this method can improve the reconstruction of concave regions for human postures and texture mapping.</b:BIBTEX_Abstract><b:BIBTEX_KeyWords> gesture recognition, image motion analysis, image reconstruction, image segmentation, image texture convex body parts, human body part segmentation, human postures, image-based visual hull, texture mapping, view synthesis, virtual silhouette image</b:BIBTEX_KeyWords></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>Zhang2009</b:Tag><b:Title>Laplacian lines for real-time shape illustration</b:Title><b:Year>2009</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Zhang</b:Last><b:First>Long</b:First></b:Person><b:Person><b:Last>He</b:Last><b:First>Ying</b:First></b:Person><b:Person><b:Last>Xie</b:Last><b:First>Xuexiang</b:First></b:Person><b:Person><b:Last>Chen</b:Last><b:First>Wei</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>129-136</b:Pages><b:Publisher>ACM</b:Publisher><b:BookTitle>I3D '09: Proceedings of the 2009 symposium on Interactive 3D graphics and games</b:BookTitle><b:BIBTEX_Abstract>This paper presents a novel object-space line drawing algorithm that can depict shape with view dependent feature lines in real-time. Strongly inspired by the Laplacian-of-Gaussian (LoG) edge detector in image processing, we define Laplacian Lines as the zero-crossing points of the Laplacian of the surface illumination. Compared to other view dependent features, Laplacian lines are computationally efficient because most expensive computations can be pre-processed. Thus, Laplacian lines are very promising for interactively illustrating large-scale models.</b:BIBTEX_Abstract></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>Aguiar2005</b:Tag><b:Title>Reconstructing Human Shape and Motion from Multi-View Video</b:Title><b:Year>2005</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Aguiar</b:Last><b:Middle>de</b:Middle><b:First>Edilson</b:First></b:Person><b:Person><b:Last>Theobalt</b:Last><b:First>Christian</b:First></b:Person><b:Person><b:Last>Magnor</b:Last><b:First>Marcus</b:First></b:Person><b:Person><b:Last>Seidel</b:Last><b:First>Hans-Peter</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>42-49</b:Pages><b:Publisher>The IEE</b:Publisher><b:BookTitle>2nd European Conference on Visual Media Production (CVMP)</b:BookTitle><b:BIBTEX_Abstract>In model-based free-viewpoint video, a detailed representation of the time-varying geometry of a real-word scene is used to generate renditions of it from novel viewpoints. In this paper, we present a method for reconstructing such a dynamic geometry model of a human actor from multi-view video. In a two-step procedure, ?rst the spatio-temporally consistent shape and poses of a generic human body model are estimated by means of a silhouette-based analysis-by-synthesis method.
In a second step, subtle details in surface geometry that are speci?c to each particular time step are recovered by enforcing a color-consistency criterion. By this means, we generate a realistic representation of the time-varying geometry of a
moving person that also reproduces these dynamic surface variations.</b:BIBTEX_Abstract></b:Source><b:Source><b:SourceType>JournalArticle</b:SourceType><b:Tag>Au2008</b:Tag><b:Title>Skeleton Extraction by Mesh Contraction</b:Title><b:Year>2008</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Au</b:Last><b:Middle>Kin-Chung</b:Middle><b:First>Oscar</b:First></b:Person><b:Person><b:Last>Tai</b:Last><b:First>Chiew-Lan</b:First></b:Person><b:Person><b:Last>Chu</b:Last><b:First>Hung-Kuo</b:First></b:Person><b:Person><b:Last>Cohen-Or</b:Last><b:First>Daniel</b:First></b:Person><b:Person><b:Last>Lee</b:Last><b:First>Tong-Yee</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>10</b:Pages><b:Volume>27</b:Volume><b:JournalName>ACM Transactions on Graphics</b:JournalName><b:BIBTEX_Abstract>Extraction of curve-skeletons is a fundamental problem with many applications in computer graphics and visualization. In this paper, we present a simple and robust skeleton extraction method based on mesh contraction. The method works directly on the mesh domain, without pre-sampling the mesh model into a volumetric representation. The method first contracts the mesh geometry into a zero-volume skeletal shape by applying implicit Laplacian smoothing with global positional constraints. The contraction does not alter the mesh connectivity and retains the key features of the original mesh. The contracted mesh is then converted into a 1D curve-skeleton through a connectivity surgery process to remove all the collapsed faces while preserving the shape of the contracted mesh and the original topology. The centeredness of the skeleton is refined by exploiting the induced skeleton-mesh mapping. The contraction process generates valuable information about the object's geometry, in particular, the skeleton-vertex correspondence and the local thickness, which are useful for various applications. We demonstrate its effectiveness in mesh segmentation and skinning animation.</b:BIBTEX_Abstract></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>Laurentini1999</b:Tag><b:Title>The visual hull of curved objects</b:Title><b:Year>1999</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Laurentini</b:Last><b:First>A.</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>356-361 vol.1</b:Pages><b:Volume>1</b:Volume><b:BookTitle>Computer Vision</b:BookTitle><b:JournalName>Computer Vision, 1999. The Proceedings of the Seventh IEEE International Conference on</b:JournalName><b:BIBTEX_Abstract>The visual hull is a geometric tool which relates the 3D shape of a concave object to its silhouettes or shadows. This paper deals with the computation of the visual hull of objects bounded by smooth curved surfaces. We show that the surfaces which bound the visual hull are surfaces relevant for the construction of the aspect graph of the object, and that the algorithms for computing the aspect graph of curved objects can be exploited for computing their visual hull</b:BIBTEX_Abstract><b:BIBTEX_KeyWords>image reconstruction, surface fittingaspect graph, concave object, curved objects, curved surfaces, visual hull</b:BIBTEX_KeyWords></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>citeulike:582023</b:Tag><b:Title>Spherical Matching for Temporal Correspondence of Non-Rigid Surfaces</b:Title><b:Year>2005</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Starck</b:Last><b:First>J.</b:First></b:Person><b:Person><b:Last>Hilton</b:Last><b:First>A.</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>1387-1394</b:Pages><b:BookTitle>IEEE International Conference on Computer Vision (ICCV)</b:BookTitle><b:JournalName>IEEE International Conference on Computer Vision (ICCV)</b:JournalName><b:BIBTEX_Abstract>This paper introduces spherical matching to estimate dense temporal correspondence of non-rigid surfaces with genus-zero topology. The spherical domain gives a consistent 2D parameterisation of non-rigid surfaces for matching. Non-rigid 3D surface correspondence is formulated as the recovery of a bijective mapping between two surfaces in the 2D domain. Formulating matching as a 2D bijection guarantees a continuous one-to-one surface correspondence without overfolding. This overcomes limitations of direct estimation of non-rigid surface correspondence in the 3D domain. A multiple resolution coarse-to-?ne algorithm is introduced to robustly estimate the dense correspondence which minimises the disparity in shape and appearance between two surfaces. 
Spherical matching is applied to derive the temporal correspondence between non-rigid surfaces reconstructed at successive frames from multiple view video sequences of people. Dense surface correspondence is recovered across complete motion sequences for both textured and uniform regions, without the requirement for a prior model of hu man shape or kinematic structure for tracking.</b:BIBTEX_Abstract><b:BIBTEX_KeyWords>starck</b:BIBTEX_KeyWords></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>Laszlo2005</b:Tag><b:Title>Predictive feedback for interactive control of physics-based characters</b:Title><b:Year>2005</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Laszlo</b:Last><b:First>Joe</b:First></b:Person><b:Person><b:Last>Neff</b:Last><b:First>Michael</b:First></b:Person><b:Person><b:Last>Singh</b:Last><b:First>Karan</b:First></b:Person></b:NameList></b:Author></b:Author><b:BookTitle>In Eurographics</b:BookTitle><b:BIBTEX_Abstract>Interactive control of a physically simulated character is a challenging problem, due both to the complexity of controlling multiple degrees of freedom with lower dimensional input and because many interesting motions lie on the fringes of character stability. This paper addresses these problems using a novel technique called predictive feedback, where a glimpse into the near future for a few sample inputs is continuously presented to the animator. We discuss issues related to the spatio-temporal distribution of predictions so that they provide meaningful and timely feedback to an animator interactively controlling a physics-based character with simple input devices, like a mouse or keyboard. We propose a visual presentation of this predictive feedback in which control input samples are chosen in the proximity of the user’s current input and the predicted results are co-located with the position of the input necessary to achieve them. We further show how the predictive samples may be automatically interpolated to control aspects of the character’s motion, such as balance, thereby freeing the animator to focus on other details. The paper thus contributes a technique for physically simulated characters that simplifies interactive character control and increases the range of motion that can be performed by both novices and experts. Many of the presented concepts extend beyond our specific input device and dynamic character control setting to more general input tasks. Categories and Subject Descriptors (according to ACM CCS): I.3.3 [Computer Graphics]: Interaction, Physical simulation, Character animation</b:BIBTEX_Abstract></b:Source><b:Source><b:SourceType>JournalArticle</b:SourceType><b:Tag>Bottino2003</b:Tag><b:Title>Introducing a new problem: shape-from-silhouette when the relative positions of the viewpoints is unknown</b:Title><b:Year>2003</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Bottino</b:Last><b:First>A.</b:First></b:Person><b:Person><b:Last>Laurentini</b:Last><b:First>A.</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>1484-1493</b:Pages><b:Volume>25</b:Volume><b:JournalName>Pattern Analysis and Machine Intelligence, IEEE Transactions on</b:JournalName><b:BIBTEX_Abstract> 3D shapes can be reconstructed from 2D silhouettes by back-projecting them from the corresponding viewpoints and intersecting the resulting solid cones. However, in many practical cases as observing an aircraft or an asteroid, the positions of the viewpoints with respect to the object are not known. In these cases, the relative position of the solid cones is not known and the intersection cannot be performed. The purpose of this paper is introducing and stating in a theoretical framework the problem of understanding 3D shapes from silhouettes when the relative positions of the viewpoints are unknown. The results presented provide a first insight into the problem. In particular, the case of orthographic viewing directions parallel to the same plane is thoroughly discussed, and sets of inequalities are presented which allow determining objects compatible with the silhouettes.</b:BIBTEX_Abstract><b:BIBTEX_KeyWords> computer vision, image reconstruction computer vision, object reconstruction, shape-from-silhouette, viewpoints, visual hull, volume intersection</b:BIBTEX_KeyWords></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>Kampel2002</b:Tag><b:Title>Octree-based fusion of shape from silhouette and shape from structured light</b:Title><b:Year>2002</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Kampel</b:Last><b:First>M.</b:First></b:Person><b:Person><b:Last>Tosovic</b:Last><b:First>S.</b:First></b:Person><b:Person><b:Last>Sablatnig</b:Last><b:First>R.</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>754-757</b:Pages><b:BookTitle>3D Data Processing Visualization and Transmission, 2002. Proceedings. First International Symposium on</b:BookTitle><b:JournalName>3D Data Processing Visualization and Transmission, 2002. Proceedings. First International Symposium on</b:JournalName><b:BIBTEX_Abstract> An algorithm for the automatic construction of a 3d model of archaeological vessels using two different 3d algorithms is presented. In archeology the determination of the exact volume of arbitrary vessels is of importance since this provides information about the manufacturer and the usage of the vessel. To acquire the 3d shape of objects with handles is complicated, since occlusions of the object's surface are introduced by the handle and can only be resolved by taking multiple views. Therefore, the 3d reconstruction is based on a sequence of images of the object taken from different viewpoints with different algorithms; shape from silhouette and shape from structured light. The output of both algorithms are then used to construct a single 3d model. Results of the algorithm developed are presented for both synthetic and real input images.</b:BIBTEX_Abstract><b:BIBTEX_KeyWords> archaeology, octrees, sensor fusion, virtual reality arbitrary vessels, archaeological vessels, automatic construction, images sequences, octree-based fusion, real input images, shape from silhouette, shape from structured light, synthetic images</b:BIBTEX_KeyWords></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>Curless1996</b:Tag><b:Title>A volumetric method for building complex models from range images</b:Title><b:Year>1996</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Curless</b:Last><b:First>Brian</b:First></b:Person><b:Person><b:Last>Levoy</b:Last><b:First>Marc</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>303-312</b:Pages><b:Publisher>ACM</b:Publisher><b:BookTitle>SIGGRAPH '96: Proceedings of the 23rd annual conference on Computer graphics and interactive techniques</b:BookTitle><b:BIBTEX_Abstract>A number of techniques have been developed for reconstructing surfaces by integrating groups of aligned range images. A desirable set of properties for such algorithms includes: incremental updating, representation of directional uncertainty, the ability to fill gaps in the reconstruction, and robustness in the presence of outliers. Prior algorithms possess subsets of these properties. In this paper, we present a volumetric method for integrating range images that possesses all of these properties.

Our volumetric representation consists of a cumulative weighted signed distance function. Working with one range image at a time, we first scan-convert it to a distance function, then combine this with the data already acquired using a simple additive scheme. To achieve space efficiency, we employ a run-length encoding of the volume. To achieve time efficiency, we resample the range image to align with the voxel grid and traverse the range and voxel scanlines synchronously. We generate the final manifold by extracting an isosurface from the volumetric grid. We show that under certain assumptions, this isosurface is optimal in the least squares sense. To fill gaps in the model, we tessellate over the boundaries between regions seen to be empty and regions never observed.

Using this method, we are able to integrate a large number of range images (as many as 70) yielding seamless, high-detail models of up to 2.6 million triangles.</b:BIBTEX_Abstract></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>Aguiar2004</b:Tag><b:Title>M3 : Marker-free Model Reconstruction and Motion Tracking from 3D Voxel Data</b:Title><b:Year>2004</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Aguiar</b:Last><b:Middle>de</b:Middle><b:First>Edilson</b:First></b:Person><b:Person><b:Last>Theobalt</b:Last><b:First>Christian</b:First></b:Person><b:Person><b:Last>Magnor</b:Last><b:First>Marcus</b:First></b:Person><b:Person><b:Last>Theisel</b:Last><b:First>Holger</b:First></b:Person><b:Person><b:Last>Seidel</b:Last><b:First>Hans-Peter</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>101-110</b:Pages><b:Publisher>IEEE</b:Publisher><b:BookTitle>12th Pacific Conference on Computer Graphics and Applications, PG 2004</b:BookTitle><b:ConferenceName>IEEE</b:ConferenceName><b:BIBTEX_Abstract>In computer animation, human motion capture from video is a widely used technique to acquire motion parameters. The acquisition process typically requires an intrusion into the scene in the form of optical markers which are used to estimate the parameters of motion as well as the kinematic structure of the performer. Marker-free optical motion capture approaches exist, but due to their dependence on a specific type of a-priori model they can hardly be used to track other subjects, e.g. animals. To bridge the gap between the generality of marker-based methods and the applicability of marker-free methods we study a flexible nonintrusive approach that estimates both, a kinematic model and its parameters of motion from a sequence of voxel-volumes. The volume sequences are reconstructed from multi-view video data by means of a shape from-silhouette technique. The method [1] is well-suited for but not limited to motion capture of human subjects, as presented in [2].</b:BIBTEX_Abstract></b:Source><b:Source><b:SourceType>JournalArticle</b:SourceType><b:Tag>Franco2009</b:Tag><b:Title>Efficient Polyhedral Modeling from Silhouettes</b:Title><b:Year>2009</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Franco</b:Last><b:First>J.-S.</b:First></b:Person><b:Person><b:Last>Boyer</b:Last><b:First>E.</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>414-427</b:Pages><b:Volume>31</b:Volume><b:JournalName>Pattern Analysis and Machine Intelligence, IEEE Transactions on</b:JournalName><b:BIBTEX_Abstract>Modeling from silhouettes is a popular and useful topic in computer vision. Many methods exist to compute the surface of the visual hull from silhouettes, but few address the problem of ensuring good topological properties of the surface, such as manifoldness. This article provides an efficient algorithm to compute such a surface in the form of a polyhedral mesh. It relies on a small number of geometric operations to compute a visual hull polyhedron in a single pass. Such simplicity enables the algorithm to combine the advantages of being fast, producing pixel-exact surfaces, and repeatably yield manifold and watertight polyhedra in general experimental conditions with real data, as verified with all datasets tested. The algorithm is fully described, its complexity analyzed and modeling results given.</b:BIBTEX_Abstract><b:BIBTEX_KeyWords>computational complexity, computational geometry, computer vision, mesh generation, solid modelling, surface fittingcomputational complexity, computer vision, geometric operation, polyhedral mesh, silhouette polyhedral modeling, topological property, visual hull surface</b:BIBTEX_KeyWords></b:Source><b:Source><b:SourceType>JournalArticle</b:SourceType><b:Tag>Zhang1999</b:Tag><b:Title>Shape from Shading: A survey</b:Title><b:Year>1999</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Zhang</b:Last><b:First>Ruo</b:First></b:Person><b:Person><b:Last>Tsai</b:Last><b:Middle>sing</b:Middle><b:First>Ping</b:First></b:Person><b:Person><b:Last>Cryer</b:Last><b:Middle>Edwin</b:Middle><b:First>James</b:First></b:Person><b:Person><b:Last>Shah</b:Last><b:First>Mubarak</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>690-706</b:Pages><b:Volume>21</b:Volume><b:JournalName>IEEE Transactions on Pattern Analysis and Machine Intelligence</b:JournalName><b:BIBTEX_Abstract>AbstractÐSince the first shape-from-shading (SFS) technique was developed by Horn in the early 1970s, many different approaches have emerged. In this paper, six well-known SFS algorithms are implemented and compared. The performance of the algorithms was analyzed on synthetic images using mean and standard deviation of depth (Z) error, mean of surface gradient (p, q) error, and CPU timing. Each algorithm works well for certain images, but performs poorly for others. In general, minimization approaches are more robust, while the other approaches are faster. The implementation of these algorithms in C and images used in this paper are available by anonymous ftp under the pub/tech_paper/survey directory at eustis.cs.ucf.edu (132.170.108.42). These are also part of the electronic version of paper. Index TermsÐ Shape from shading, analysis of algorithms, Lambertian model, survey of shape from shading algorithms. 1</b:BIBTEX_Abstract></b:Source><b:Source><b:SourceType>JournalArticle</b:SourceType><b:Tag>Laurentini1994</b:Tag><b:Title>The visual hull concept for silhouette-based image understanding </b:Title><b:Year>1994</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Laurentini</b:Last><b:First>A.</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>150-162</b:Pages><b:Volume>16</b:Volume><b:JournalName>Pattern Analysis and Machine Intelligence, IEEE Transactions on</b:JournalName><b:BIBTEX_Abstract>Many algorithms for both identifying and reconstructing a 3-D object are based on the 2-D silhouettes of the object. In general, identifying a nonconvex object using a silhouette-based approach implies neglecting some features of its surface as identification clues. The same features cannot be reconstructed by volume intersection techniques using multiple silhouettes of the object. This paper addresses the problem of finding which parts of a nonconvex object are relevant for silhouette-based image understanding. For this purpose, the geometric concept of visual hull of a 3-D object is introduced. This is the closest approximation of object S that can be obtained with the volume intersection approach; it is the maximal object silhouette-equivalent to S, i.e., which can be substituted for S without affecting any silhouette. Only the parts of the surface of S that also lie on the surface of the visual hull can be reconstructed or identified using silhouette-based algorithms. The visual hull depends not only on the object but also on the region allowed to the viewpoint. Two main viewing regions result in the external and internal visual hull. In the former case the viewing region is related to the convex hull of S, in the latter it is bounded by S. The internal visual hull also admits an interpretation not related to silhouettes. Algorithms for computing visual hulls are presented and their complexity analyzed. In general, the visual hull of a 3-D planar face object turns out to be bounded by planar and curved patches</b:BIBTEX_Abstract><b:BIBTEX_KeyWords>image reconstructionexternal visual hull, internal visual hull, nonconvex object, object identification, object reconstruction, silhouette-based image understanding</b:BIBTEX_KeyWords></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>Thormahlen2008</b:Tag><b:Title>3D-modeling by ortho-image generation from image sequences</b:Title><b:Year>2008</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Thorm\"{a}hlen</b:Last><b:First>Thorsten</b:First></b:Person><b:Person><b:Last>Seidel</b:Last><b:First>Hans-Peter</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>1-5</b:Pages><b:Publisher>ACM</b:Publisher><b:BookTitle>SIGGRAPH '08: ACM SIGGRAPH 2008 papers</b:BookTitle><b:BIBTEX_Abstract>This paper introduces an approach to performance animation that employs video cameras and a small set of retro-reflective markers to create a low-cost, easy-to-use system that might someday be practical for home use. The low-dimensional control signals from the user's performance are supplemented by a database of pre-recorded human motion. At run time, the system automatically learns a series of local models from a set of motion capture examples that are a close match to the marker locations captured by the cameras. These local models are then used to reconstruct the motion of the user as a full-body animation. We demonstrate the power of this approach with real-time control of six different behaviors using two video cameras and a small set of retro-reflective markers. We compare the resulting animation to animation from commercial motion capture equipment with a full set of markers.</b:BIBTEX_Abstract></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>1073375</b:Tag><b:Title>Video-based character animation</b:Title><b:Year>2005</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Starck</b:Last><b:First>J.</b:First></b:Person><b:Person><b:Last>Miller</b:Last><b:First>G.</b:First></b:Person><b:Person><b:Last>Hilton</b:Last><b:First>A.</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>49-58</b:Pages><b:Publisher>ACM</b:Publisher><b:BookTitle>SCA '05: Proceedings of the 2005 ACM SIGGRAPH/Eurographics symposium on Computer animation</b:BookTitle><b:BIBTEX_Abstract>In this paper we introduce a video-based representation for free viewpoint visualization and motion control of 3D character models created from multiple view video sequences of real people. Previous approaches to video-based rendering provide no control of scene dynamics to manipulate, retarget, and create new 3D content from captured scenes. Here we contribute a new approach, combining image based reconstruction and video-based animation to allow controlled animation of people from captured multiple view video sequences. We represent a character as a motion graph of free viewpoint video motions for animation control. We introduce the use of geometry videos to represent reconstructed scenes of people for free viewpoint video rendering. We describe a novel spherical matching algorithm to derive global surface to surface correspondence in spherical geometry images for motion blending and the construction of seamless transitions between motion sequences. Finally, we demonstrate interactive video-based character animation with real-time rendering and free viewpoint visualization. This approach synthesizes highly realistic character animations with dynamic surface shape and appearance captured from multiple view video of people.</b:BIBTEX_Abstract></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>Park2002</b:Tag><b:Title>Automatic 3D model reconstruction using voxel coding and pose integration</b:Title><b:Year>2002</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Park</b:Last><b:First>Soon-Yong</b:First></b:Person><b:Person><b:Last>Subbarao</b:Last><b:First>M.</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages> II-533-II-536 vol.2</b:Pages><b:Volume>2</b:Volume><b:BookTitle>3D Data Processing Visualization and Transmission</b:BookTitle><b:JournalName>Image Processing. 2002. Proceedings. 2002 International Conference on</b:JournalName><b:BIBTEX_Abstract>Automatic reconstruction of a complete 3D model of a complex object is presented. The complete 3D model is reconstructed by integrating two 3D models which are reconstructed from different poses of the object. For each pose of the object, a 3D model is reconstructed by combining stereo image analysis, shape from silhouettes, and a volumetric integration technique. Stereo image analysis and shape from silhouettes techniques complement each other to reconstruct an accurate and noise-resistant 3D model. For a reliable volumetric integration of multiple partial shapes, a voxel coding technique is introduced. The voxel coding technique facilitates a selection of consistent partial shapes for shape integration. In order to reconstruct all visible surfaces of a complex object with concavities and holes, two 3D models from different poses of the object are reconstructed and integrated to obtain the complete 3D model. A voxel coding technique is again used during pose integration. Experimental results on a real object demonstrate that our approach has advantages and is effective.</b:BIBTEX_Abstract><b:BIBTEX_KeyWords> binary codes, image reconstruction, stereo image processing, virtual reality automatic reconstruction, complete 3D model, complex object, multiple partial shapes, partial shapes, poses, shape integration, silhouettes, stereo image analysis, volumetric integration technique, voxel coding technique</b:BIBTEX_KeyWords></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>You2005</b:Tag><b:Title>Wavelet-Based Approach for Skeleton Extraction</b:Title><b:Year>2005</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>You</b:Last><b:First>Xinge</b:First></b:Person><b:Person><b:Last>Fang</b:Last><b:First>Bin</b:First></b:Person><b:Person><b:Last>Tang</b:Last><b:Middle>Yan</b:Middle><b:First>Yuan</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>228-233</b:Pages><b:Publisher>IEEE Computer Society</b:Publisher><b:BookTitle>WACV-MOTION '05: Proceedings of the Seventh IEEE Workshops on Application of Computer Vision (WACV/MOTION'05) - Volume 1</b:BookTitle><b:BIBTEX_Abstract>We propose a novel wavelet-based approach to extract skeleton of ribbon-like shapes based on a new concept called Wavelet-based Local Modulus Maxima Symmetry (WLMMS). The development of the new approach benefits from the desirable properties of the constructed new wavelet function. Based on the proposed WLMMS, initial skeleton of the regular region of ribbon-like shape are computed. Special attention is given to development of a new amendment technique which is called interpolation compensation. It is used to to remove artifacts of the initial skeletons and compute the skeletons in the singular region of the shape. Experimental results show that the proposed approach is not only capable of extracting precisely the skeleton of the ribbon-like shape with the low computational cost, but also robust against noise. And the proposed algorithm is applicable to both binary and gray scale image, as most traditional methods fail to do.</b:BIBTEX_Abstract></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>Lin2008</b:Tag><b:Title>3D reconstruction by combining shape from silhouette with stereo</b:Title><b:Year>2008</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Lin</b:Last><b:First>Huei-Yung</b:First></b:Person><b:Person><b:Last>Wu</b:Last><b:First>Jing-Ren</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>1-4</b:Pages><b:BookTitle>Pattern Recognition</b:BookTitle><b:JournalName>Pattern Recognition, 2008. ICPR 2008. 19th International Conference on</b:JournalName><b:BIBTEX_Abstract>In this paper we propose a 3D reconstruction algorithm by combining shape from silhouette with stereo. Visual hull of the object is first derived from multi-view silhouette images. Pairwise stereo matching for shape refinement is then accomplished using the best viewable images. Based on the reduced correspondence searching range constrained by contact points and bounding edges, significant improvement of visual hull is possible even if the number of cameras is limited. Experimental results are presented for both synthetic data and real scene images.</b:BIBTEX_Abstract><b:BIBTEX_KeyWords>image matching, image reconstruction, stereo image processingmultiview silhouette images, object visual hull, pairwise stereo matching, reconstruction algorithm, shape refinement</b:BIBTEX_KeyWords></b:Source><b:Source><b:SourceType>JournalArticle</b:SourceType><b:Tag>Cornea2005</b:Tag><b:Title>Computing hierarchical curve-skeletons of 3D objects</b:Title><b:Year>2005</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Cornea</b:Last><b:Middle>D.</b:Middle><b:First>Nicu</b:First></b:Person><b:Person><b:Last>Silver</b:Last><b:First>Deborah</b:First></b:Person><b:Person><b:Last>Yuan</b:Last><b:First>Xiaosong</b:First></b:Person><b:Person><b:Last>Balasubramanian</b:Last><b:First>Raman</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>945-955</b:Pages><b:Volume>21</b:Volume><b:JournalName>The Visual Computer</b:JournalName><b:BIBTEX_Abstract>A curve-skeleton of a 3D object is a stick-like figure or centerline representation of that object. It is used for diverse applications, including virtual colonoscopy and animation. In this paper, we introduce the concept of hierarchical curve-skeletons and describe a general and robust methodology that computes a family of increasingly detailed curve-skeletons. The algorithm is based upon computing a repulsive force field over a discretization of the 3D object and using topological characteristics of the resulting vector field, such as critical points and critical curves, to extract the curve-skeleton. We demonstrate this method on many different types of 3D objects (volumetric, polygonal and scattered point sets) and discuss various extensions of this approach.</b:BIBTEX_Abstract></b:Source><b:Source><b:SourceType>JournalArticle</b:SourceType><b:Tag>Shiratori2008</b:Tag><b:Title>Accelerometer-based User Interfaces for the Control of a Physically Simulated Character</b:Title><b:Year>2008</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Shiratori</b:Last><b:First>Takaaki</b:First></b:Person><b:Person><b:Last>Hodgins</b:Last><b:Middle>K.</b:Middle><b:First>Jessica</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>1-9</b:Pages><b:Volume>27</b:Volume><b:JournalName>ACM Transactions on Graphics (SIGGRAPH Asia 2008)</b:JournalName><b:BIBTEX_Abstract>In late 2006, Nintendo released a new game controller, the Wiimote, which included a three-axis accelerometer. Since then, a large variety of novel applications for these controllers have been developed by both independent and commercial developers. We add to this growing library with three performance interfaces that allow the user to control the motion of a dynamically simulated, animated character through the motion of his or her arms, wrists, or legs. For comparison, we also implement a traditional joystick/button interface. We assess these interfaces by having users test them on a set of tracks containing turns and pits. Two of the interfaces (legs and wrists) were judged to be more immersive and were better liked than the joystick/button interface by our subjects. All three of the Wiimote interfaces provided better control than the joystick interface based on an analysis of the failures seen during the user study.</b:BIBTEX_Abstract><b:BIBTEX_KeyWords>Wii</b:BIBTEX_KeyWords></b:Source><b:Source><b:SourceType>ConferenceProceedings</b:SourceType><b:Tag>Zhang2004</b:Tag><b:Title>Spacetime Faces: High-Resolution Capture for Modeling and Animation</b:Title><b:Year>2004</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Zhang</b:Last><b:First>Li</b:First></b:Person><b:Person><b:Last>Snavely</b:Last><b:First>Noah</b:First></b:Person><b:Person><b:Last>Curless</b:Last><b:First>Brian</b:First></b:Person><b:Person><b:Last>Seitz</b:Last><b:Middle>M.</b:Middle><b:First>Steven</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>548-558</b:Pages><b:BookTitle>ACM Annual Conference on Computer Graphics</b:BookTitle><b:BIBTEX_Abstract>We present an end-to-end system that goes from video sequences to high resolution, editable, dynamically controllable face models. The capture system employs synchronized video cameras and structured light projectors to record videos of a moving face from multiple viewpoints. A novel spacetime stereo algorithm is introduced to compute depth maps accurately and overcome over-fitting deficiencies in prior work. A new template fitting and tracking procedure fills in missing data and yields point correspondence across the entire sequence without using markers. We demonstrate a data-driven, interactive method for inverse kinematics that draws on the large set of fitted templates and allows for posing new expressions by dragging surface points directly. Finally, we describe new tools that model the dynamics in the input sequence to enable new animations, created via key-framing or texture-synthesis techniques.</b:BIBTEX_Abstract></b:Source><b:Source><b:SourceType>JournalArticle</b:SourceType><b:Tag>Rusinkiewicz2002</b:Tag><b:Title>Real-time 3D model acquisition</b:Title><b:Year>2002</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Rusinkiewicz</b:Last><b:First>Szymon</b:First></b:Person><b:Person><b:Last>Hall-Holt</b:Last><b:First>Olaf</b:First></b:Person><b:Person><b:Last>Levoy</b:Last><b:First>Marc</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>438-446</b:Pages><b:Volume>21</b:Volume><b:Publisher>ACM</b:Publisher><b:JournalName>ACM Trans. Graph.</b:JournalName><b:BIBTEX_Abstract>The digitization of the 3D shape of real objects is a rapidly expanding field, with applications in entertainment, design, and archaeology. We propose a new 3D model acquisition system that permits the user to rotate an object by hand and see a continuously-updated model as the object is scanned. This tight feedback loop allows the user to find and fill holes in the model in real time, and determine when the object has been completely covered. Our system is based on a 60 Hz. structured-light rangefinder, a real-time variant of ICP (iterative closest points) for alignment, and point-based merging and rendering algorithms. We demonstrate the ability of our prototype to scan objects faster and with greater ease than conventional model acquisition pipelines.</b:BIBTEX_Abstract></b:Source><b:Source><b:SourceType>JournalArticle</b:SourceType><b:Tag>Lee2002</b:Tag><b:Title>Interactive control of avatars animated with human motion data</b:Title><b:Year>2002</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Lee</b:Last><b:First>Jehee</b:First></b:Person><b:Person><b:Last>Chai</b:Last><b:First>Jinxiang</b:First></b:Person><b:Person><b:Last>A.</b:Last><b:Middle>S.</b:Middle><b:First>Paul</b:First></b:Person><b:Person><b:Last>Hodgins</b:Last><b:Middle>K.</b:Middle><b:First>Jessica</b:First></b:Person><b:Person><b:Last>Pollard</b:Last><b:Middle>S.</b:Middle><b:First>Nancy</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>491-500</b:Pages><b:Volume>21</b:Volume><b:Publisher>ACM</b:Publisher><b:JournalName>ACM Trans. Graph.</b:JournalName><b:BIBTEX_Abstract>Real-time control of three-dimensional avatars is an important problem in the context of computer games and virtual environments. Avatar animation and control is difficult, however, because a large repertoire of avatar behaviors must be made available, and the user must be able to select from this set of behaviors, possibly with a low-dimensional input device. One appealing approach to obtaining a rich set of avatar behaviors is to collect an extended, unlabeled sequence of motion data appropriate to the application. In this paper, we show that such a motion database can be preprocessed for flexibility in behavior and efficient search and exploited for real-time avatar control. Flexibility is created by identifying plausible transitions between motion segments, and efficient search through the resulting graph structure is obtained through clustering. Three interface techniques are demonstrated for controlling avatar motion using this data structure: the user selects from a set of available choices, sketches a path through an environment, or acts out a desired motion in front of a video camera. We demonstrate the flexibility of the approach through four different applications and compare the avatar motion to directly recorded human motion.</b:BIBTEX_Abstract></b:Source><b:Source><b:SourceType>JournalArticle</b:SourceType><b:Tag>Park2006</b:Tag><b:Title>Capturing and animating skin deformation in human motion</b:Title><b:Year>2006</b:Year><b:Author><b:Author><b:NameList><b:Person><b:Last>Park</b:Last><b:Middle>Il</b:Middle><b:First>Sang</b:First></b:Person><b:Person><b:Last>Hodgins</b:Last><b:Middle>K.</b:Middle><b:First>Jessica</b:First></b:Person></b:NameList></b:Author></b:Author><b:Pages>881-889</b:Pages><b:Volume>25</b:Volume><b:Publisher>ACM</b:Publisher><b:JournalName>ACM Trans. Graph.</b:JournalName><b:BIBTEX_Abstract>During dynamic activities, the surface of the human body moves in many subtle but visually significant ways: bending, bulging, jiggling, and stretching. We present a technique for capturing and animating those motions using a commercial motion capture system and approximately 350 markers. Although the number of markers is significantly larger than that used in conventional motion capture, it is only a sparse representation of the true shape of the body. We supplement this sparse sample with a detailed, actor-specific surface model. The motion of the skin can then be computed by segmenting the markers into the motion of a set of rigid parts and a residual deformation (approximated first as a quadratic transformation and then with radial basis functions). We demonstrate the power of this approach by capturing flexing muscles, high frequency motions, and abrupt decelerations on several actors. We compare these results both to conventional motion capture and skinning and to synchronized video of the actors.</b:BIBTEX_Abstract></b:Source></b:Sources>
